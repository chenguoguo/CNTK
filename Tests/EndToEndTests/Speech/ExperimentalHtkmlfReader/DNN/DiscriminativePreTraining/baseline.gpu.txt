=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true reader=[readerType=ExperimentalHTKMLFReader] reader=[prefetch=true]
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
05/03/2016 18:17:08: -------------------------------------------------------------------
05/03/2016 18:17:08: Build info: 

05/03/2016 18:17:08: 		Built time: May  3 2016 17:56:15
05/03/2016 18:17:08: 		Last modified date: Tue May  3 11:36:22 2016
05/03/2016 18:17:08: 		Build type: release
05/03/2016 18:17:08: 		Build target: GPU
05/03/2016 18:17:08: 		With 1bit-SGD: no
05/03/2016 18:17:08: 		Math lib: acml
05/03/2016 18:17:08: 		CUDA_PATH: /usr/local/cuda-7.5
05/03/2016 18:17:08: 		CUB_PATH: /usr/local/cub-1.4.1
05/03/2016 18:17:08: 		CUDNN_PATH: /usr/local/cudnn-4.0
05/03/2016 18:17:08: 		Build Branch: HEAD
05/03/2016 18:17:08: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
05/03/2016 18:17:08: 		Built by philly on 18750d26eb32
05/03/2016 18:17:08: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
05/03/2016 18:17:08: -------------------------------------------------------------------

05/03/2016 18:17:08: Running on localhost at 2016/05/03 18:17:08
05/03/2016 18:17:08: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining  OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true  reader=[readerType=ExperimentalHTKMLFReader]  reader=[prefetch=true]



05/03/2016 18:17:08: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
05/03/2016 18:17:08: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true
reader=[readerType=ExperimentalHTKMLFReader]
reader=[prefetch=true]

05/03/2016 18:17:08: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

05/03/2016 18:17:08: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
05/03/2016 18:17:08: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true
reader=[readerType=ExperimentalHTKMLFReader]
reader=[prefetch=true]

05/03/2016 18:17:08: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

05/03/2016 18:17:08: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
] [readerType=ExperimentalHTKMLFReader] [prefetch=true]

configparameters: cntk_dpt.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/DiscriminativePreTraining/../../../DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
05/03/2016 18:17:08: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
05/03/2016 18:17:08: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
05/03/2016 18:17:08: Precision = "float"
05/03/2016 18:17:08: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
05/03/2016 18:17:08: CNTKCommandTrainInfo: dptPre1 : 2
05/03/2016 18:17:08: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
05/03/2016 18:17:08: CNTKCommandTrainInfo: dptPre2 : 2
05/03/2016 18:17:08: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
05/03/2016 18:17:08: CNTKCommandTrainInfo: speechTrain : 4
05/03/2016 18:17:08: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

05/03/2016 18:17:08: ##############################################################################
05/03/2016 18:17:08: #                                                                            #
05/03/2016 18:17:08: # Action "train"                                                             #
05/03/2016 18:17:08: #                                                                            #
05/03/2016 18:17:08: ##############################################################################

05/03/2016 18:17:08: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
Reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MLFDataDeserializer::MLFDataDeserializer: read 948 utterances

05/03/2016 18:17:08: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 18:17:08: Created model with 19 nodes on GPU 0.

05/03/2016 18:17:08: Training criterion node(s):
05/03/2016 18:17:08: 	ce = CrossEntropyWithSoftmax

05/03/2016 18:17:08: Evaluation criterion node(s):

05/03/2016 18:17:08: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x26a6618: {[features Value[363 x *]] }
0x273c4a8: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x273c668: {[ce Value[1]] }
0x273c968: {[logPrior Value[132 x 1]] }
0x2f6ef18: {[globalMean Value[363 x 1]] }
0x2f73ed8: {[labels Value[132 x *]] }
0x33c7658: {[globalInvStd Value[363 x 1]] }
0x33c8048: {[globalPrior Value[132 x 1]] }
0x33c8a78: {[HL1.W Value[512 x 363]] }
0x33c9ed8: {[HL1.b Value[512 x 1]] }
0x33cb178: {[OL.W Value[132 x 512]] }
0x33cb9a8: {[OL.b Value[132 x 1]] }
0x34c15e8: {[err Value[1]] }
0x34c2bb8: {[featNorm Value[363 x *]] }
0x34c3128: {[HL1.t Value[512 x *]] }
0x34c34d8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x34c3638: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x34c37f8: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x34c39b8: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x34c44c8: {[ce Gradient[1]] }
0x34c4688: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x34c4848: {[OL.t Gradient[132 x 1 x *]] }
0x34c4a08: {[OL.b Gradient[132 x 1]] }

05/03/2016 18:17:08: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 18:17:08: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples

05/03/2016 18:17:08: Starting minibatch loop.
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.74183846 * 2560; err = 0.80195313 * 2560; time = 0.2032s; samplesPerSecond = 12596.3
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.91124763 * 2560; err = 0.70898438 * 2560; time = 0.0132s; samplesPerSecond = 194632.4
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.58015976 * 2560; err = 0.66640625 * 2560; time = 0.0127s; samplesPerSecond = 200973.5
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.27427139 * 2560; err = 0.58750000 * 2560; time = 0.0129s; samplesPerSecond = 198326.6
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.05503616 * 2560; err = 0.56093750 * 2560; time = 0.0130s; samplesPerSecond = 197105.0
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.91055145 * 2560; err = 0.52812500 * 2560; time = 0.0130s; samplesPerSecond = 197668.1
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81562653 * 2560; err = 0.51171875 * 2560; time = 0.0127s; samplesPerSecond = 201511.3
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.68803253 * 2560; err = 0.48476562 * 2560; time = 0.0148s; samplesPerSecond = 172902.9
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.57382050 * 2560; err = 0.45429687 * 2560; time = 0.0128s; samplesPerSecond = 200250.3
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62090149 * 2560; err = 0.47304687 * 2560; time = 0.0127s; samplesPerSecond = 201352.8
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59272461 * 2560; err = 0.47500000 * 2560; time = 0.0126s; samplesPerSecond = 202371.5
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51520386 * 2560; err = 0.44531250 * 2560; time = 0.0123s; samplesPerSecond = 207421.8
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.49181976 * 2560; err = 0.45039062 * 2560; time = 0.0127s; samplesPerSecond = 201940.5
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53703613 * 2560; err = 0.44804688 * 2560; time = 0.0125s; samplesPerSecond = 204882.0
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.43095398 * 2560; err = 0.41640625 * 2560; time = 0.0119s; samplesPerSecond = 214783.1
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.41503601 * 2560; err = 0.40078125 * 2560; time = 0.0121s; samplesPerSecond = 211570.2
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38913574 * 2560; err = 0.41132812 * 2560; time = 0.0117s; samplesPerSecond = 219309.5
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41207886 * 2560; err = 0.42226562 * 2560; time = 0.0113s; samplesPerSecond = 226890.0
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.39968262 * 2560; err = 0.40664062 * 2560; time = 0.0112s; samplesPerSecond = 228164.0
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42729187 * 2560; err = 0.42617187 * 2560; time = 0.0118s; samplesPerSecond = 217520.6
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.41336365 * 2560; err = 0.42343750 * 2560; time = 0.0117s; samplesPerSecond = 218784.7
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.33186951 * 2560; err = 0.39960937 * 2560; time = 0.0119s; samplesPerSecond = 215488.2
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.28581238 * 2560; err = 0.38710937 * 2560; time = 0.0118s; samplesPerSecond = 217594.6
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34127502 * 2560; err = 0.40976563 * 2560; time = 0.0118s; samplesPerSecond = 216637.0
05/03/2016 18:17:08:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.32666016 * 2560; err = 0.39726563 * 2560; time = 0.0121s; samplesPerSecond = 211465.4
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.21437378 * 2560; err = 0.37265625 * 2560; time = 0.0117s; samplesPerSecond = 218002.2
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23749695 * 2560; err = 0.37343750 * 2560; time = 0.0117s; samplesPerSecond = 218467.3
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29956665 * 2560; err = 0.39023438 * 2560; time = 0.0116s; samplesPerSecond = 220328.8
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.21198120 * 2560; err = 0.37382813 * 2560; time = 0.0118s; samplesPerSecond = 216088.5
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20528259 * 2560; err = 0.36718750 * 2560; time = 0.0116s; samplesPerSecond = 220918.2
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23613586 * 2560; err = 0.37343750 * 2560; time = 0.0119s; samplesPerSecond = 214945.4
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25615234 * 2560; err = 0.38164063 * 2560; time = 0.0119s; samplesPerSecond = 215180.3
05/03/2016 18:17:09: Finished Epoch[ 1 of 2]: [Training] ce = 1.62945061 * 81920; err = 0.46030273 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.626423s
05/03/2016 18:17:09: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

05/03/2016 18:17:09: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples

05/03/2016 18:17:09: Starting minibatch loop.
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.23230953 * 2560; err = 0.38320312 * 2560; time = 0.0127s; samplesPerSecond = 201400.4
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.20511341 * 2560; err = 0.37421875 * 2560; time = 0.0118s; samplesPerSecond = 217446.7
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28783760 * 2560; err = 0.37421875 * 2560; time = 0.0121s; samplesPerSecond = 212148.8
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.22809334 * 2560; err = 0.37421875 * 2560; time = 0.0118s; samplesPerSecond = 216490.5
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.18090286 * 2560; err = 0.35468750 * 2560; time = 0.0121s; samplesPerSecond = 211797.8
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.28175354 * 2560; err = 0.37695312 * 2560; time = 0.0118s; samplesPerSecond = 217391.3
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.22251205 * 2560; err = 0.37382813 * 2560; time = 0.0117s; samplesPerSecond = 218766.0
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17863007 * 2560; err = 0.36328125 * 2560; time = 0.0122s; samplesPerSecond = 210682.2
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23061218 * 2560; err = 0.35742188 * 2560; time = 0.0120s; samplesPerSecond = 212695.2
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18048782 * 2560; err = 0.37578125 * 2560; time = 0.0116s; samplesPerSecond = 220537.6
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.19648056 * 2560; err = 0.35976562 * 2560; time = 0.0115s; samplesPerSecond = 223463.7
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18896942 * 2560; err = 0.35429688 * 2560; time = 0.0115s; samplesPerSecond = 221760.2
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16628113 * 2560; err = 0.35937500 * 2560; time = 0.0119s; samplesPerSecond = 214891.3
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12856445 * 2560; err = 0.35195312 * 2560; time = 0.0117s; samplesPerSecond = 219422.3
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10083466 * 2560; err = 0.32617188 * 2560; time = 0.0118s; samplesPerSecond = 216362.4
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09875183 * 2560; err = 0.33906250 * 2560; time = 0.0117s; samplesPerSecond = 218299.7
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.18634949 * 2560; err = 0.35820313 * 2560; time = 0.0120s; samplesPerSecond = 213582.5
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.15709991 * 2560; err = 0.35195312 * 2560; time = 0.0117s; samplesPerSecond = 218169.4
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10971069 * 2560; err = 0.34960938 * 2560; time = 0.0118s; samplesPerSecond = 217465.2
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11317139 * 2560; err = 0.35000000 * 2560; time = 0.0123s; samplesPerSecond = 208435.1
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.08727722 * 2560; err = 0.32578125 * 2560; time = 0.0119s; samplesPerSecond = 215815.2
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.12296143 * 2560; err = 0.34101562 * 2560; time = 0.0122s; samplesPerSecond = 210128.9
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12966003 * 2560; err = 0.35078125 * 2560; time = 0.0123s; samplesPerSecond = 208639.0
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27489319 * 2560; err = 0.39257812 * 2560; time = 0.0118s; samplesPerSecond = 216453.9
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.17423401 * 2560; err = 0.35156250 * 2560; time = 0.0118s; samplesPerSecond = 216179.7
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13240051 * 2560; err = 0.35625000 * 2560; time = 0.0126s; samplesPerSecond = 203174.6
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.13792114 * 2560; err = 0.34335938 * 2560; time = 0.0124s; samplesPerSecond = 206651.6
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13433228 * 2560; err = 0.33710937 * 2560; time = 0.0124s; samplesPerSecond = 207203.6
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.05835876 * 2560; err = 0.33710937 * 2560; time = 0.0265s; samplesPerSecond = 96647.5
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09596558 * 2560; err = 0.33476563 * 2560; time = 0.0195s; samplesPerSecond = 130966.4
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08180847 * 2560; err = 0.33242187 * 2560; time = 0.0162s; samplesPerSecond = 158024.7
05/03/2016 18:17:09:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06572876 * 2560; err = 0.33632812 * 2560; time = 0.0141s; samplesPerSecond = 181792.4
05/03/2016 18:17:09: Finished Epoch[ 2 of 2]: [Training] ce = 1.16156273 * 81920; err = 0.35460205 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.417437s
05/03/2016 18:17:09: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
05/03/2016 18:17:09: CNTKCommandTrainEnd: dptPre1

05/03/2016 18:17:09: Action "train" complete.


05/03/2016 18:17:09: ##############################################################################
05/03/2016 18:17:09: #                                                                            #
05/03/2016 18:17:09: # Action "edit"                                                              #
05/03/2016 18:17:09: #                                                                            #
05/03/2016 18:17:09: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


05/03/2016 18:17:09: Action "edit" complete.


05/03/2016 18:17:09: ##############################################################################
05/03/2016 18:17:09: #                                                                            #
05/03/2016 18:17:09: # Action "train"                                                             #
05/03/2016 18:17:09: #                                                                            #
05/03/2016 18:17:09: ##############################################################################

05/03/2016 18:17:09: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
Reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MLFDataDeserializer::MLFDataDeserializer: read 948 utterances

05/03/2016 18:17:09: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 18:17:09: Loaded model with 24 nodes on GPU 0.

05/03/2016 18:17:09: Training criterion node(s):
05/03/2016 18:17:09: 	ce = CrossEntropyWithSoftmax

05/03/2016 18:17:09: Evaluation criterion node(s):

05/03/2016 18:17:09: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7f09fe822b88: {[HL1.b Value[512 x 1]] }
0x7f09fe8250e8: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7f09fe8252f8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7f09fe82d588: {[globalPrior Value[132 x 1]] }
0x7f09fe8b3ee8: {[globalMean Value[363 x 1]] }
0x7f09fe8c77d8: {[HL2.b Value[512 x 1]] }
0x7f0a1061b298: {[OL.b Value[132 x 1]] }
0x7f0a1062b958: {[labels Value[132 x *3]] }
0x7f0a10644d48: {[features Value[363 x *3]] }
0x7f0a10647f68: {[OL.t Gradient[132 x 1 x *3]] }
0x7f0a1066e278: {[logPrior Value[132 x 1]] }
0x7f0a1067c5a8: {[ce Gradient[1]] }
0x7f0a1067c768: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7f0a10688948: {[OL.b Gradient[132 x 1]] }
0x7f0a1068a938: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7f0a1068aaf8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0x7f0a1068acb8: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7f0a1068c9b8: {[HL1.W Value[512 x 363]] }
0x7f0a1068dea8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7f0a1068e068: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7f0a1069a978: {[globalInvStd Value[363 x 1]] }
0x7f0a106a1318: {[OL.W Value[132 x 512]] }
0x7f0a106a18b8: {[HL2.W Value[512 x 512]] }
0x7f0a106a2b58: {[HL1.t Value[512 x *3]] }
0x7f0a106a6ec8: {[err Value[1]] }
0x7f0a106a7088: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7f0a106a7248: {[ce Value[1]] }
0x7f0a106aa8f8: {[featNorm Value[363 x *3]] }

05/03/2016 18:17:09: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 18:17:09: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples

05/03/2016 18:17:09: Starting minibatch loop.
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.30124588 * 2560; err = 0.80703125 * 2560; time = 0.1030s; samplesPerSecond = 24849.8
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.75448074 * 2560; err = 0.69960937 * 2560; time = 0.0143s; samplesPerSecond = 178995.9
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.20926208 * 2560; err = 0.58515625 * 2560; time = 0.0142s; samplesPerSecond = 179939.6
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88578110 * 2560; err = 0.50117188 * 2560; time = 0.0142s; samplesPerSecond = 180497.8
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.71906204 * 2560; err = 0.47773437 * 2560; time = 0.0142s; samplesPerSecond = 179674.3
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.60130463 * 2560; err = 0.44648437 * 2560; time = 0.0142s; samplesPerSecond = 180269.0
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.56077118 * 2560; err = 0.45000000 * 2560; time = 0.0142s; samplesPerSecond = 180485.1
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47116547 * 2560; err = 0.42460938 * 2560; time = 0.0142s; samplesPerSecond = 180434.2
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.38874512 * 2560; err = 0.40781250 * 2560; time = 0.0142s; samplesPerSecond = 179926.9
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41911163 * 2560; err = 0.42539063 * 2560; time = 0.0143s; samplesPerSecond = 179171.3
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.38730774 * 2560; err = 0.42148438 * 2560; time = 0.0141s; samplesPerSecond = 181021.1
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36617889 * 2560; err = 0.41015625 * 2560; time = 0.0143s; samplesPerSecond = 179636.5
05/03/2016 18:17:09:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.33381653 * 2560; err = 0.40781250 * 2560; time = 0.0143s; samplesPerSecond = 178970.9
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.39802246 * 2560; err = 0.40546875 * 2560; time = 0.0145s; samplesPerSecond = 176527.4
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33336182 * 2560; err = 0.40195313 * 2560; time = 0.0143s; samplesPerSecond = 178471.8
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.33834229 * 2560; err = 0.40195313 * 2560; time = 0.0145s; samplesPerSecond = 176090.2
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.26663208 * 2560; err = 0.37578125 * 2560; time = 0.0144s; samplesPerSecond = 178148.9
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28086243 * 2560; err = 0.39296875 * 2560; time = 0.0144s; samplesPerSecond = 177248.5
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29481506 * 2560; err = 0.39531250 * 2560; time = 0.0144s; samplesPerSecond = 178372.4
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27625122 * 2560; err = 0.39375000 * 2560; time = 0.0144s; samplesPerSecond = 177322.2
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.26905518 * 2560; err = 0.38984375 * 2560; time = 0.0164s; samplesPerSecond = 155793.6
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21494751 * 2560; err = 0.36250000 * 2560; time = 0.0141s; samplesPerSecond = 181174.8
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.20699158 * 2560; err = 0.36914062 * 2560; time = 0.0141s; samplesPerSecond = 181046.7
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.25002136 * 2560; err = 0.37851563 * 2560; time = 0.0142s; samplesPerSecond = 179851.1
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.22617187 * 2560; err = 0.37656250 * 2560; time = 0.0142s; samplesPerSecond = 180154.8
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.14840393 * 2560; err = 0.35468750 * 2560; time = 0.0142s; samplesPerSecond = 180803.7
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16649780 * 2560; err = 0.35468750 * 2560; time = 0.0142s; samplesPerSecond = 180218.2
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.22885742 * 2560; err = 0.36992188 * 2560; time = 0.0143s; samplesPerSecond = 179586.1
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.16533203 * 2560; err = 0.36484375 * 2560; time = 0.0144s; samplesPerSecond = 178285.4
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17502136 * 2560; err = 0.35664062 * 2560; time = 0.0145s; samplesPerSecond = 176466.5
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.16159058 * 2560; err = 0.35195312 * 2560; time = 0.0145s; samplesPerSecond = 176296.4
05/03/2016 18:17:10:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.17113953 * 2560; err = 0.35429688 * 2560; time = 0.0144s; samplesPerSecond = 178074.6
05/03/2016 18:17:10: Finished Epoch[ 1 of 2]: [Training] ce = 1.49907970 * 81920; err = 0.42547607 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.591209s
05/03/2016 18:17:10: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

05/03/2016 18:17:10: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples

05/03/2016 18:17:10: Starting minibatch loop.
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.14215403 * 2560; err = 0.34882812 * 2560; time = 0.0171s; samplesPerSecond = 149393.1
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.17049246 * 2560; err = 0.36328125 * 2560; time = 0.0164s; samplesPerSecond = 156116.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.24373856 * 2560; err = 0.37460938 * 2560; time = 0.0143s; samplesPerSecond = 178633.7
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.18655586 * 2560; err = 0.36445312 * 2560; time = 0.0146s; samplesPerSecond = 175607.1
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13848000 * 2560; err = 0.35039063 * 2560; time = 0.0145s; samplesPerSecond = 177162.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.21884155 * 2560; err = 0.36757812 * 2560; time = 0.0145s; samplesPerSecond = 176856.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14372940 * 2560; err = 0.35000000 * 2560; time = 0.0144s; samplesPerSecond = 177975.5
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12769089 * 2560; err = 0.34960938 * 2560; time = 0.0146s; samplesPerSecond = 175270.4
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14114227 * 2560; err = 0.33554688 * 2560; time = 0.0145s; samplesPerSecond = 176710.2
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12445145 * 2560; err = 0.34843750 * 2560; time = 0.0145s; samplesPerSecond = 176868.9
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14137878 * 2560; err = 0.34101562 * 2560; time = 0.0144s; samplesPerSecond = 178273.0
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.12705154 * 2560; err = 0.33867188 * 2560; time = 0.0145s; samplesPerSecond = 177040.1
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10779419 * 2560; err = 0.34531250 * 2560; time = 0.0145s; samplesPerSecond = 176187.2
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.07003021 * 2560; err = 0.32500000 * 2560; time = 0.0147s; samplesPerSecond = 174732.1
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.05308990 * 2560; err = 0.31406250 * 2560; time = 0.0146s; samplesPerSecond = 175330.5
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.06392975 * 2560; err = 0.33085938 * 2560; time = 0.0145s; samplesPerSecond = 176917.8
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.14430847 * 2560; err = 0.35507813 * 2560; time = 0.0145s; samplesPerSecond = 176698.0
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.14809570 * 2560; err = 0.35859375 * 2560; time = 0.0144s; samplesPerSecond = 177383.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.08184509 * 2560; err = 0.33515625 * 2560; time = 0.0145s; samplesPerSecond = 176430.0
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.07637024 * 2560; err = 0.33359375 * 2560; time = 0.0144s; samplesPerSecond = 177605.1
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.06249695 * 2560; err = 0.32500000 * 2560; time = 0.0145s; samplesPerSecond = 177015.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09361877 * 2560; err = 0.33320312 * 2560; time = 0.0166s; samplesPerSecond = 154216.9
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12118683 * 2560; err = 0.34843750 * 2560; time = 0.0145s; samplesPerSecond = 177027.9
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13457642 * 2560; err = 0.35195312 * 2560; time = 0.0145s; samplesPerSecond = 176698.0
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.09024963 * 2560; err = 0.33984375 * 2560; time = 0.0145s; samplesPerSecond = 176759.0
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07457275 * 2560; err = 0.33164063 * 2560; time = 0.0142s; samplesPerSecond = 179775.3
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05975952 * 2560; err = 0.32070312 * 2560; time = 0.0144s; samplesPerSecond = 177420.5
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09778137 * 2560; err = 0.33242187 * 2560; time = 0.0144s; samplesPerSecond = 177260.8
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.01963196 * 2560; err = 0.32539062 * 2560; time = 0.0144s; samplesPerSecond = 178012.7
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.07533875 * 2560; err = 0.33515625 * 2560; time = 0.0144s; samplesPerSecond = 177666.7
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06417236 * 2560; err = 0.33007812 * 2560; time = 0.0145s; samplesPerSecond = 177064.6
05/03/2016 18:17:10:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04990234 * 2560; err = 0.33359375 * 2560; time = 0.0144s; samplesPerSecond = 177814.8
05/03/2016 18:17:10: Finished Epoch[ 2 of 2]: [Training] ce = 1.11232681 * 81920; err = 0.34179688 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.475359s
05/03/2016 18:17:10: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
05/03/2016 18:17:10: CNTKCommandTrainEnd: dptPre2

05/03/2016 18:17:10: Action "train" complete.


05/03/2016 18:17:10: ##############################################################################
05/03/2016 18:17:10: #                                                                            #
05/03/2016 18:17:10: # Action "edit"                                                              #
05/03/2016 18:17:10: #                                                                            #
05/03/2016 18:17:10: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


05/03/2016 18:17:10: Action "edit" complete.


05/03/2016 18:17:10: ##############################################################################
05/03/2016 18:17:10: #                                                                            #
05/03/2016 18:17:10: # Action "train"                                                             #
05/03/2016 18:17:10: #                                                                            #
05/03/2016 18:17:10: ##############################################################################

05/03/2016 18:17:10: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
Reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MLFDataDeserializer::MLFDataDeserializer: read 948 utterances

05/03/2016 18:17:10: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 18:17:10: Loaded model with 29 nodes on GPU 0.

05/03/2016 18:17:10: Training criterion node(s):
05/03/2016 18:17:10: 	ce = CrossEntropyWithSoftmax

05/03/2016 18:17:10: Evaluation criterion node(s):

05/03/2016 18:17:10: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7f09fe827098: {[HL3.b Value[512 x 1]] }
0x7f09fe89e7f8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7f09fe89e9b8: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7f09fe89eb78: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7f09fe89edd8: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7f09fe89ef98: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7f09fe89f158: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7f09fe89f318: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7f09fe8b0138: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7f09fe8df7b8: {[globalInvStd Value[363 x 1]] }
0x7f09fe8fcda8: {[globalPrior Value[132 x 1]] }
0x7f09fe8fd7b8: {[OL.b Gradient[132 x 1]] }
0x7f0a09400cb8: {[labels Value[132 x *6]] }
0x7f0a09401948: {[features Value[363 x *6]] }
0x7f0a10602098: {[ce Value[1]] }
0x7f0a10602368: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7f0a10602528: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7f0a10602e28: {[HL1.W Value[512 x 363]] }
0x7f0a10621db8: {[HL2.b Value[512 x 1]] }
0x7f0a10622178: {[err Value[1]] }
0x7f0a10622338: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7f0a10628688: {[HL1.b Value[512 x 1]] }
0x7f0a10629088: {[globalMean Value[363 x 1]] }
0x7f0a1062a238: {[HL2.W Value[512 x 512]] }
0x7f0a10640818: {[HL1.t Value[512 x *6]] }
0x7f0a10644918: {[ce Gradient[1]] }
0x7f0a10644ad8: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7f0a10644c98: {[OL.t Gradient[132 x 1 x *6]] }
0x7f0a106add08: {[OL.W Value[132 x 512]] }
0x7f0a106addb8: {[OL.b Value[132 x 1]] }
0x7f0a106b1c08: {[featNorm Value[363 x *6]] }
0x7f0a106b1cc8: {[logPrior Value[132 x 1]] }
0x7f0a106b28a8: {[HL3.W Value[512 x 512]] }

05/03/2016 18:17:10: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 18:17:10: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples

05/03/2016 18:17:11: Starting minibatch loop.
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.97086372 * 2560; err = 0.81445312 * 2560; time = 0.1083s; samplesPerSecond = 23627.4
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.63975792 * 2560; err = 0.63320312 * 2560; time = 0.0185s; samplesPerSecond = 138625.7
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02565231 * 2560; err = 0.54257813 * 2560; time = 0.0186s; samplesPerSecond = 137597.4
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74204865 * 2560; err = 0.47500000 * 2560; time = 0.0189s; samplesPerSecond = 135571.7
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.58343964 * 2560; err = 0.45156250 * 2560; time = 0.0187s; samplesPerSecond = 136876.4
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47893143 * 2560; err = 0.42343750 * 2560; time = 0.0185s; samplesPerSecond = 138468.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43405457 * 2560; err = 0.40898438 * 2560; time = 0.0187s; samplesPerSecond = 137162.5
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35973663 * 2560; err = 0.39648438 * 2560; time = 0.0186s; samplesPerSecond = 137412.8
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.28108978 * 2560; err = 0.37968750 * 2560; time = 0.0187s; samplesPerSecond = 136854.5
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.29773560 * 2560; err = 0.39765625 * 2560; time = 0.0186s; samplesPerSecond = 137471.8
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.28441925 * 2560; err = 0.39062500 * 2560; time = 0.0187s; samplesPerSecond = 136979.0
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27777252 * 2560; err = 0.38164063 * 2560; time = 0.0187s; samplesPerSecond = 136869.1
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23615112 * 2560; err = 0.37421875 * 2560; time = 0.0187s; samplesPerSecond = 137199.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.31171112 * 2560; err = 0.38671875 * 2560; time = 0.0187s; samplesPerSecond = 137199.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.25573883 * 2560; err = 0.37773438 * 2560; time = 0.0187s; samplesPerSecond = 137074.3
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.27382965 * 2560; err = 0.38398437 * 2560; time = 0.0187s; samplesPerSecond = 137045.0
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20634155 * 2560; err = 0.36406250 * 2560; time = 0.0187s; samplesPerSecond = 137199.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20973816 * 2560; err = 0.36562500 * 2560; time = 0.0187s; samplesPerSecond = 137169.8
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.20688782 * 2560; err = 0.36718750 * 2560; time = 0.0187s; samplesPerSecond = 136942.3
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20260315 * 2560; err = 0.37226562 * 2560; time = 0.0187s; samplesPerSecond = 137037.6
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.20553894 * 2560; err = 0.37187500 * 2560; time = 0.0188s; samplesPerSecond = 136322.5
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14160156 * 2560; err = 0.34726563 * 2560; time = 0.0192s; samplesPerSecond = 133097.6
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.15316467 * 2560; err = 0.35273437 * 2560; time = 0.0191s; samplesPerSecond = 133702.4
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.19352417 * 2560; err = 0.35468750 * 2560; time = 0.0187s; samplesPerSecond = 137125.7
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.17192078 * 2560; err = 0.35937500 * 2560; time = 0.0184s; samplesPerSecond = 138791.0
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.08281860 * 2560; err = 0.33867188 * 2560; time = 0.0190s; samplesPerSecond = 134432.6
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.11028442 * 2560; err = 0.34453125 * 2560; time = 0.0190s; samplesPerSecond = 134467.9
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.17454224 * 2560; err = 0.35312500 * 2560; time = 0.0185s; samplesPerSecond = 138648.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.11068115 * 2560; err = 0.34531250 * 2560; time = 0.0183s; samplesPerSecond = 140274.0
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.12955627 * 2560; err = 0.34296875 * 2560; time = 0.0189s; samplesPerSecond = 135092.3
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12482300 * 2560; err = 0.34570312 * 2560; time = 0.0191s; samplesPerSecond = 133835.2
05/03/2016 18:17:11:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12771912 * 2560; err = 0.34453125 * 2560; time = 0.0185s; samplesPerSecond = 138745.9
05/03/2016 18:17:11: Finished Epoch[ 1 of 4]: [Training] ce = 1.40639620 * 81920; err = 0.40274658 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.730805s
05/03/2016 18:17:11: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

05/03/2016 18:17:11: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples

05/03/2016 18:17:11: Starting minibatch loop.
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.51739798 * 5120; err = 0.41425781 * 5120; time = 0.0371s; samplesPerSecond = 138083.6
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.25793447 * 5120; err = 0.37539062 * 5120; time = 0.0293s; samplesPerSecond = 174887.3
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.18638287 * 5120; err = 0.36718750 * 5120; time = 0.0293s; samplesPerSecond = 174803.7
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12794571 * 5120; err = 0.34218750 * 5120; time = 0.0292s; samplesPerSecond = 175198.5
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14070625 * 5120; err = 0.34570312 * 5120; time = 0.0291s; samplesPerSecond = 176163.0
05/03/2016 18:17:11:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.14582825 * 5120; err = 0.34765625 * 5120; time = 0.0290s; samplesPerSecond = 176320.7
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.11193542 * 5120; err = 0.34414062 * 5120; time = 0.0291s; samplesPerSecond = 175721.6
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08574600 * 5120; err = 0.33789062 * 5120; time = 0.0293s; samplesPerSecond = 174714.2
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.21058807 * 5120; err = 0.37363281 * 5120; time = 0.0293s; samplesPerSecond = 174791.8
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09668579 * 5120; err = 0.34335938 * 5120; time = 0.0292s; samplesPerSecond = 175192.5
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05845032 * 5120; err = 0.32675781 * 5120; time = 0.0292s; samplesPerSecond = 175372.5
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10728455 * 5120; err = 0.34726563 * 5120; time = 0.0292s; samplesPerSecond = 175625.2
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08716888 * 5120; err = 0.33593750 * 5120; time = 0.0290s; samplesPerSecond = 176838.3
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06778870 * 5120; err = 0.31855469 * 5120; time = 0.0291s; samplesPerSecond = 176023.7
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04079590 * 5120; err = 0.32910156 * 5120; time = 0.0292s; samplesPerSecond = 175372.5
05/03/2016 18:17:12:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06249542 * 5120; err = 0.32968750 * 5120; time = 0.0333s; samplesPerSecond = 153823.0
05/03/2016 18:17:12: Finished Epoch[ 2 of 4]: [Training] ce = 1.14407091 * 81920; err = 0.34866943 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.48398s
05/03/2016 18:17:12: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

05/03/2016 18:17:12: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples

05/03/2016 18:17:12: Starting minibatch loop.
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.11238871 * 5120; err = 0.34804687 * 5120; time = 0.0305s; samplesPerSecond = 167780.8
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09456148 * 5120; err = 0.34121094 * 5120; time = 0.0291s; samplesPerSecond = 176187.2
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.10800076 * 5120; err = 0.34667969 * 5120; time = 0.0290s; samplesPerSecond = 176728.5
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.16617966 * 5120; err = 0.35566406 * 5120; time = 0.0290s; samplesPerSecond = 176765.1
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14173546 * 5120; err = 0.34550781 * 5120; time = 0.0292s; samplesPerSecond = 175396.5
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07876053 * 5120; err = 0.33359375 * 5120; time = 0.0297s; samplesPerSecond = 172297.8
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08043213 * 5120; err = 0.33437500 * 5120; time = 0.0297s; samplesPerSecond = 172251.4
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07423630 * 5120; err = 0.33007812 * 5120; time = 0.0297s; samplesPerSecond = 172292.0
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02659454 * 5120; err = 0.31113281 * 5120; time = 0.0297s; samplesPerSecond = 172338.3
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04602737 * 5120; err = 0.31855469 * 5120; time = 0.0298s; samplesPerSecond = 171817.8
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05524902 * 5120; err = 0.33613281 * 5120; time = 0.0296s; samplesPerSecond = 172722.1
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07627411 * 5120; err = 0.33613281 * 5120; time = 0.0296s; samplesPerSecond = 172862.0
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05101776 * 5120; err = 0.31660156 * 5120; time = 0.0297s; samplesPerSecond = 172669.6
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03016815 * 5120; err = 0.32480469 * 5120; time = 0.0298s; samplesPerSecond = 171864.0
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04644623 * 5120; err = 0.32929687 * 5120; time = 0.0296s; samplesPerSecond = 173171.9
05/03/2016 18:17:12:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02751465 * 5120; err = 0.32265625 * 5120; time = 0.0294s; samplesPerSecond = 174339.4
05/03/2016 18:17:12: Finished Epoch[ 3 of 4]: [Training] ce = 1.07597418 * 81920; err = 0.33315430 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.477846s
05/03/2016 18:17:12: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

05/03/2016 18:17:12: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples

05/03/2016 18:17:12: Starting minibatch loop.
05/03/2016 18:17:12:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03003817 * 5120; err = 0.31289062 * 5120; time = 0.0315s; samplesPerSecond = 162519.0
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04547925 * 4926; err = 0.32947625 * 4926; time = 0.1533s; samplesPerSecond = 32130.6
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01249580 * 5120; err = 0.32246094 * 5120; time = 0.0298s; samplesPerSecond = 171593.3
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.99796486 * 5120; err = 0.31425781 * 5120; time = 0.0319s; samplesPerSecond = 160491.5
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 0.99781761 * 5120; err = 0.31464844 * 5120; time = 0.0297s; samplesPerSecond = 172187.7
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00107079 * 5120; err = 0.31855469 * 5120; time = 0.0297s; samplesPerSecond = 172106.6
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.02518806 * 5120; err = 0.31972656 * 5120; time = 0.0480s; samplesPerSecond = 106584.5
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00891876 * 5120; err = 0.31660156 * 5120; time = 0.0300s; samplesPerSecond = 170740.7
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99774780 * 5120; err = 0.30585937 * 5120; time = 0.0299s; samplesPerSecond = 171174.5
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.00037842 * 5120; err = 0.30722656 * 5120; time = 0.0300s; samplesPerSecond = 170894.5
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.02586746 * 5120; err = 0.31816406 * 5120; time = 0.0299s; samplesPerSecond = 171031.5
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06024628 * 5120; err = 0.33574219 * 5120; time = 0.0300s; samplesPerSecond = 170541.6
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98301010 * 5120; err = 0.30214844 * 5120; time = 0.0299s; samplesPerSecond = 171168.8
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.96488800 * 5120; err = 0.30156250 * 5120; time = 0.0299s; samplesPerSecond = 171065.8
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.99069977 * 5120; err = 0.31640625 * 5120; time = 0.0300s; samplesPerSecond = 170866.0
05/03/2016 18:17:13:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.97961731 * 5120; err = 0.29921875 * 5120; time = 0.0299s; samplesPerSecond = 171403.7
05/03/2016 18:17:13: Finished Epoch[ 4 of 4]: [Training] ce = 1.00739784 * 81920; err = 0.31477051 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.629907s
05/03/2016 18:17:13: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
05/03/2016 18:17:13: CNTKCommandTrainEnd: speechTrain

05/03/2016 18:17:13: Action "train" complete.

05/03/2016 18:17:13: __COMPLETED__