=== Running mpiexec -n 3 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN/cntk.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu DeviceId=0 timestamping=true speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]] speechTrain=[reader=[prefetch=true]] numCPUThreads=8 precision=double speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]] speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]] speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]] speechTrain=[SGD=[maxEpochs=4]] speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]] stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
MPIWrapper: initializing MPI
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 3 nodes pinging each other
ping [requestnodes (before change)]: 3 nodes pinging each other
ping [requestnodes (before change)]: 3 nodes pinging each other
ping [requestnodes (before change)]: all 3 nodes responded
requestnodes [MPIWrapper]: using 3 out of 3 MPI nodes (3 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 3 nodes pinging each other
ping [requestnodes (before change)]: all 3 nodes responded
requestnodes [MPIWrapper]: using 3 out of 3 MPI nodes (3 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 3 nodes pinging each other
ping [requestnodes (before change)]: all 3 nodes responded
requestnodes [MPIWrapper]: using 3 out of 3 MPI nodes (3 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 3 nodes pinging each other
ping [requestnodes (after change)]: all 3 nodes responded
mpihelper: we are cog 1 in a gearbox of 3
ping [mpihelper]: 3 nodes pinging each other
ping [requestnodes (after change)]: all 3 nodes responded
mpihelper: we are cog 0 in a gearbox of 3
ping [mpihelper]: 3 nodes pinging each other
ping [requestnodes (after change)]: all 3 nodes responded
mpihelper: we are cog 2 in a gearbox of 3
ping [mpihelper]: 3 nodes pinging each other
ping [mpihelper]: all 3 nodes responded
ping [mpihelper]: all 3 nodes responded
ping [mpihelper]: all 3 nodes responded
05/03/2016 18:17:50: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr_speechTrain.logrank0
05/03/2016 18:17:50: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr_speechTrain.logrank1
05/03/2016 18:17:51: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr_speechTrain.logrank2
--------------------------------------------------------------------------
mpiexec has exited due to process rank 0 with PID 14269 on
node 87698aadbc9d exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.

--------------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:17:50: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:17:50: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: 		Built time: May  3 2016 17:56:15
MPI Rank 0: 05/03/2016 18:17:50: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 0: 05/03/2016 18:17:50: 		Build type: release
MPI Rank 0: 05/03/2016 18:17:50: 		Build target: GPU
MPI Rank 0: 05/03/2016 18:17:50: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 18:17:50: 		Math lib: acml
MPI Rank 0: 05/03/2016 18:17:50: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 05/03/2016 18:17:50: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 05/03/2016 18:17:50: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 05/03/2016 18:17:50: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 18:17:50: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 0: 05/03/2016 18:17:50: 		Built by philly on 18750d26eb32
MPI Rank 0: 05/03/2016 18:17:50: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 05/03/2016 18:17:50: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Running on localhost at 2016/05/03 18:17:50
MPI Rank 0: 05/03/2016 18:17:50: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN  OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DeviceId=0  timestamping=true  speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]  speechTrain=[reader=[prefetch=true]]  numCPUThreads=8  precision=double  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]  speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]  speechTrain=[SGD=[maxEpochs=4]]  speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]  stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:17:50: precision = "float"
MPI Rank 0: command = speechTrain
MPI Rank 0: deviceId = $DeviceId$
MPI Rank 0: parallelTrain = true
MPI Rank 0: speechTrain = [
MPI Rank 0:     action = "train"
MPI Rank 0:     modelPath = "$RunDir$/models/cntkSpeech.dnn"
MPI Rank 0:     deviceId = $DeviceId$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SimpleNetworkBuilder = [
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 0:         evalCriterion = "ErrorPrediction"
MPI Rank 0:         layerTypes = "Sigmoid"
MPI Rank 0:         initValueScale = 1.0
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         uniformInit = true
MPI Rank 0:         needPrior = true
MPI Rank 0:     ]
MPI Rank 0:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = 'CE'
MPI Rank 0:         evalCriterion = 'Err'
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 0:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 0:         featNorm = if applyMeanVarNorm
MPI Rank 0:                    then MeanVarNorm(features)
MPI Rank 0:                    else features
MPI Rank 0:         layers[layer:1..L-1] = if layer > 1
MPI Rank 0:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 0:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 0:         CE = if trainingCriterion == 'CE'
MPI Rank 0:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 0:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 0:         Err = if evalCriterion == 'Err' then
MPI Rank 0:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 0:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 0:         logPrior = LogPrior(labels)
MPI Rank 0:         // TODO: how to add a tag to an infix operation?
MPI Rank 0:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 0:     ]
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize = 20480
MPI Rank 0:         minibatchSize = 64:256:1024
MPI Rank 0:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 0:         numMBsToShowResult = 10
MPI Rank 0:         momentumPerMB = 0.9:0.656119
MPI Rank 0:         dropoutRate = 0.0
MPI Rank 0:         maxEpochs = 3
MPI Rank 0:         keepCheckPointFiles = true
MPI Rank 0:         clippingThresholdPerSample = 1#INF
MPI Rank 0:         ParallelTrain = [
MPI Rank 0:             parallelizationMethod = "DataParallelSGD"
MPI Rank 0:             distributedMBReading = true
MPI Rank 0:             DataParallelSGD = [
MPI Rank 0:                 gradientBits = 32
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0:         AutoAdjust = [
MPI Rank 0:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 0:             loadBestModel = true
MPI Rank 0:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 0:             learnRateDecreaseFactor = 0.5
MPI Rank 0:             learnRateIncreaseFactor = 1.382
MPI Rank 0:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0:     reader = [
MPI Rank 0:         readerType = "HTKMLFReader"
MPI Rank 0:         readMethod = "blockRandomize"
MPI Rank 0:         miniBatchMode = "partial"
MPI Rank 0:         randomize = "auto"
MPI Rank 0:         verbosity = 0
MPI Rank 0:         features = [
MPI Rank 0:             dim = 363
MPI Rank 0:             type = "real"
MPI Rank 0:             scpFile = "glob_0000.scp"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             mlfFile = "$DataDir$/glob_0000.mlf"
MPI Rank 0:             labelMappingFile = "$DataDir$/state.list"
MPI Rank 0:             labelDim = 132
MPI Rank 0:             labelType = "category"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 0: speechTrain=[reader=[prefetch=true]]
MPI Rank 0: numCPUThreads=8
MPI Rank 0: precision=double
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 0: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:17:50: precision = "float"
MPI Rank 0: command = speechTrain
MPI Rank 0: deviceId = 0
MPI Rank 0: parallelTrain = true
MPI Rank 0: speechTrain = [
MPI Rank 0:     action = "train"
MPI Rank 0:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 0:     deviceId = 0
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SimpleNetworkBuilder = [
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 0:         evalCriterion = "ErrorPrediction"
MPI Rank 0:         layerTypes = "Sigmoid"
MPI Rank 0:         initValueScale = 1.0
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         uniformInit = true
MPI Rank 0:         needPrior = true
MPI Rank 0:     ]
MPI Rank 0:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = 'CE'
MPI Rank 0:         evalCriterion = 'Err'
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 0:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 0:         featNorm = if applyMeanVarNorm
MPI Rank 0:                    then MeanVarNorm(features)
MPI Rank 0:                    else features
MPI Rank 0:         layers[layer:1..L-1] = if layer > 1
MPI Rank 0:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 0:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 0:         CE = if trainingCriterion == 'CE'
MPI Rank 0:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 0:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 0:         Err = if evalCriterion == 'Err' then
MPI Rank 0:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 0:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 0:         logPrior = LogPrior(labels)
MPI Rank 0:         // TODO: how to add a tag to an infix operation?
MPI Rank 0:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 0:     ]
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize = 20480
MPI Rank 0:         minibatchSize = 64:256:1024
MPI Rank 0:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 0:         numMBsToShowResult = 10
MPI Rank 0:         momentumPerMB = 0.9:0.656119
MPI Rank 0:         dropoutRate = 0.0
MPI Rank 0:         maxEpochs = 3
MPI Rank 0:         keepCheckPointFiles = true
MPI Rank 0:         clippingThresholdPerSample = 1#INF
MPI Rank 0:         ParallelTrain = [
MPI Rank 0:             parallelizationMethod = "DataParallelSGD"
MPI Rank 0:             distributedMBReading = true
MPI Rank 0:             DataParallelSGD = [
MPI Rank 0:                 gradientBits = 32
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0:         AutoAdjust = [
MPI Rank 0:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 0:             loadBestModel = true
MPI Rank 0:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 0:             learnRateDecreaseFactor = 0.5
MPI Rank 0:             learnRateIncreaseFactor = 1.382
MPI Rank 0:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0:     reader = [
MPI Rank 0:         readerType = "HTKMLFReader"
MPI Rank 0:         readMethod = "blockRandomize"
MPI Rank 0:         miniBatchMode = "partial"
MPI Rank 0:         randomize = "auto"
MPI Rank 0:         verbosity = 0
MPI Rank 0:         features = [
MPI Rank 0:             dim = 363
MPI Rank 0:             type = "real"
MPI Rank 0:             scpFile = "glob_0000.scp"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 0:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 0:             labelDim = 132
MPI Rank 0:             labelType = "category"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 0: speechTrain=[reader=[prefetch=true]]
MPI Rank 0: numCPUThreads=8
MPI Rank 0: precision=double
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 0: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 0: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: cntk.cntk:command=speechTrain
MPI Rank 0: configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 0: configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 0: configparameters: cntk.cntk:deviceId=0
MPI Rank 0: configparameters: cntk.cntk:numCPUThreads=8
MPI Rank 0: configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: configparameters: cntk.cntk:parallelTrain=true
MPI Rank 0: configparameters: cntk.cntk:precision=double
MPI Rank 0: configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 0: configparameters: cntk.cntk:speechTrain=[
MPI Rank 0:     action = "train"
MPI Rank 0:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 0:     deviceId = 0
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SimpleNetworkBuilder = [
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 0:         evalCriterion = "ErrorPrediction"
MPI Rank 0:         layerTypes = "Sigmoid"
MPI Rank 0:         initValueScale = 1.0
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         uniformInit = true
MPI Rank 0:         needPrior = true
MPI Rank 0:     ]
MPI Rank 0:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 0:         layerSizes = 363:512:512:132
MPI Rank 0:         trainingCriterion = 'CE'
MPI Rank 0:         evalCriterion = 'Err'
MPI Rank 0:         applyMeanVarNorm = true
MPI Rank 0:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 0:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 0:         featNorm = if applyMeanVarNorm
MPI Rank 0:                    then MeanVarNorm(features)
MPI Rank 0:                    else features
MPI Rank 0:         layers[layer:1..L-1] = if layer > 1
MPI Rank 0:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 0:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 0:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 0:         CE = if trainingCriterion == 'CE'
MPI Rank 0:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 0:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 0:         Err = if evalCriterion == 'Err' then
MPI Rank 0:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 0:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 0:         logPrior = LogPrior(labels)
MPI Rank 0:         // TODO: how to add a tag to an infix operation?
MPI Rank 0:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 0:     ]
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize = 20480
MPI Rank 0:         minibatchSize = 64:256:1024
MPI Rank 0:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 0:         numMBsToShowResult = 10
MPI Rank 0:         momentumPerMB = 0.9:0.656119
MPI Rank 0:         dropoutRate = 0.0
MPI Rank 0:         maxEpochs = 3
MPI Rank 0:         keepCheckPointFiles = true
MPI Rank 0:         clippingThresholdPerSample = 1#INF
MPI Rank 0:         ParallelTrain = [
MPI Rank 0:             parallelizationMethod = "DataParallelSGD"
MPI Rank 0:             distributedMBReading = true
MPI Rank 0:             DataParallelSGD = [
MPI Rank 0:                 gradientBits = 32
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0:         AutoAdjust = [
MPI Rank 0:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 0:             loadBestModel = true
MPI Rank 0:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 0:             learnRateDecreaseFactor = 0.5
MPI Rank 0:             learnRateIncreaseFactor = 1.382
MPI Rank 0:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0:     reader = [
MPI Rank 0:         readerType = "HTKMLFReader"
MPI Rank 0:         readMethod = "blockRandomize"
MPI Rank 0:         miniBatchMode = "partial"
MPI Rank 0:         randomize = "auto"
MPI Rank 0:         verbosity = 0
MPI Rank 0:         features = [
MPI Rank 0:             dim = 363
MPI Rank 0:             type = "real"
MPI Rank 0:             scpFile = "glob_0000.scp"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 0:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 0:             labelDim = 132
MPI Rank 0:             labelType = "category"
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0: ] [reader=[readerType=ExperimentalHTKMLFReader]] [reader=[prefetch=true]] [SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]] [SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]] [SGD=[ParallelTrain=[parallelizationStartEpoch=2]]] [SGD=[maxEpochs=4]] [SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 0: 
MPI Rank 0: configparameters: cntk.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 0: configparameters: cntk.cntk:timestamping=true
MPI Rank 0: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 18:17:50: Commands: speechTrain
MPI Rank 0: 05/03/2016 18:17:50: Precision = "double"
MPI Rank 0: 05/03/2016 18:17:50: Using 8 CPU threads.
MPI Rank 0: 05/03/2016 18:17:50: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn
MPI Rank 0: 05/03/2016 18:17:50: CNTKCommandTrainInfo: speechTrain : 4
MPI Rank 0: 05/03/2016 18:17:50: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 4
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: ##############################################################################
MPI Rank 0: 05/03/2016 18:17:50: #                                                                            #
MPI Rank 0: 05/03/2016 18:17:50: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 18:17:50: #                                                                            #
MPI Rank 0: 05/03/2016 18:17:50: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: CNTKCommandTrainBegin: speechTrain
MPI Rank 0: SimpleNetworkBuilder Using GPU 0
MPI Rank 0: Reading script file glob_0000.scp ... 948 entries
MPI Rank 0: HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 0: HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 0: total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
MPI Rank 0: htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MPI Rank 0: MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MPI Rank 0: MLFDataDeserializer::MLFDataDeserializer: read 948 utterances
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Creating virgin network.
MPI Rank 0: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==8
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 7 roots:
MPI Rank 0: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
MPI Rank 0: 	EvalErrorPrediction = ErrorPrediction()
MPI Rank 0: 	InvStdOfFeatures = InvStdDev()
MPI Rank 0: 	MeanOfFeatures = Mean()
MPI Rank 0: 	PosteriorProb = Softmax()
MPI Rank 0: 	Prior = Mean()
MPI Rank 0: 	ScaledLogLikelihood = Minus()
MPI Rank 0: 
MPI Rank 0: Validating network. 25 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> labels = InputValue() :  -> [132 x *]
MPI Rank 0: Validating --> W2 = LearnableParameter() :  -> [132 x 512]
MPI Rank 0: Validating --> W1 = LearnableParameter() :  -> [512 x 512]
MPI Rank 0: Validating --> W0 = LearnableParameter() :  -> [512 x 363]
MPI Rank 0: Validating --> features = InputValue() :  -> [363 x *]
MPI Rank 0: Validating --> MeanOfFeatures = Mean (features) : [363 x *] -> [363]
MPI Rank 0: Validating --> InvStdOfFeatures = InvStdDev (features) : [363 x *] -> [363]
MPI Rank 0: Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [363 x *], [363], [363] -> [363 x *]
MPI Rank 0: Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [512 x 363], [363 x *] -> [512 x *]
MPI Rank 0: Validating --> B0 = LearnableParameter() :  -> [512 x 1]
MPI Rank 0: Validating --> W0*features+B0 = Plus (W0*features, B0) : [512 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 0: Validating --> H1 = Sigmoid (W0*features+B0) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> W1*H1 = Times (W1, H1) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> B1 = LearnableParameter() :  -> [512 x 1]
MPI Rank 0: Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 0: Validating --> H2 = Sigmoid (W1*H1+B1) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> W2*H1 = Times (W2, H2) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
MPI Rank 0: Validating --> B2 = LearnableParameter() :  -> [132 x 1]
MPI Rank 0: Validating --> HLast = Plus (W2*H1, B2) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
MPI Rank 0: Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 0: Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 0: Validating --> PosteriorProb = Softmax (HLast) : [132 x 1 x *] -> [132 x 1 x *]
MPI Rank 0: Validating --> Prior = Mean (labels) : [132 x *] -> [132]
MPI Rank 0: Validating --> LogOfPrior = Log (Prior) : [132] -> [132]
MPI Rank 0: Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [132 x 1 x *], [132] -> [132 x 1 x *]
MPI Rank 0: 
MPI Rank 0: Validating network. 17 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 12 out of 25 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Created model with 25 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Training criterion node(s):
MPI Rank 0: 05/03/2016 18:17:50: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Evaluation criterion node(s):
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: 	EvalErrorPrediction = ErrorPrediction
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[EvalErrorPrediction Gradient[1]] [InvStdOfFeatures Gradient[363]] [LogOfPrior Gradient[132]] [MVNormalizedFeatures Gradient[363 x *]] [MeanOfFeatures Gradient[363]] [PosteriorProb Gradient[132 x 1 x *]] [PosteriorProb Value[132 x 1 x *]] [Prior Gradient[132]] [ScaledLogLikelihood Gradient[132 x 1 x *]] [features Gradient[363 x *]] [labels Gradient[132 x *]] }
MPI Rank 0: 0x2a87668: {[features Value[363 x *]] }
MPI Rank 0: 0x2e36ad8: {[W0 Value[512 x 363]] }
MPI Rank 0: 0x2e45e18: {[MeanOfFeatures Value[363]] }
MPI Rank 0: 0x2e46328: {[InvStdOfFeatures Value[363]] }
MPI Rank 0: 0x3792b78: {[B0 Value[512 x 1]] }
MPI Rank 0: 0x3794ce8: {[W1 Value[512 x 512]] }
MPI Rank 0: 0x3b492a8: {[B1 Value[512 x 1]] }
MPI Rank 0: 0x3b4a448: {[W2 Value[132 x 512]] }
MPI Rank 0: 0x3b4b0f8: {[B2 Value[132 x 1]] }
MPI Rank 0: 0x3b4bf28: {[labels Value[132 x *]] }
MPI Rank 0: 0x3b4d188: {[Prior Value[132]] }
MPI Rank 0: 0x3b52bb8: {[EvalErrorPrediction Value[1]] }
MPI Rank 0: 0x3b52d18: {[ScaledLogLikelihood Value[132 x 1 x *]] }
MPI Rank 0: 0x3b52ed8: {[CrossEntropyWithSoftmax Value[1]] }
MPI Rank 0: 0x3b53368: {[W0 Gradient[512 x 363]] [W0*features+B0 Value[512 x 1 x *]] }
MPI Rank 0: 0x3b53498: {[LogOfPrior Value[132]] }
MPI Rank 0: 0x3b54bf8: {[MVNormalizedFeatures Value[363 x *]] }
MPI Rank 0: 0x3b553b8: {[W0*features Value[512 x *]] }
MPI Rank 0: 0x3b555c8: {[H1 Value[512 x 1 x *]] [W0*features Gradient[512 x *]] }
MPI Rank 0: 0x3b55728: {[W0*features+B0 Gradient[512 x 1 x *]] [W1*H1 Value[512 x 1 x *]] }
MPI Rank 0: 0x3b558e8: {[W1 Gradient[512 x 512]] [W1*H1+B1 Value[512 x 1 x *]] }
MPI Rank 0: 0x3b55aa8: {[H2 Value[512 x 1 x *]] [W1*H1 Gradient[512 x 1 x *]] }
MPI Rank 0: 0x3b55c68: {[B0 Gradient[512 x 1]] [H1 Gradient[512 x 1 x *]] [W1*H1+B1 Gradient[512 x 1 x *]] [W2*H1 Value[132 x 1 x *]] }
MPI Rank 0: 0x3b55e28: {[HLast Value[132 x 1 x *]] [W2 Gradient[132 x 512]] }
MPI Rank 0: 0x3b56988: {[CrossEntropyWithSoftmax Gradient[1]] }
MPI Rank 0: 0x3b56b48: {[B1 Gradient[512 x 1]] [H2 Gradient[512 x 1 x *]] [HLast Gradient[132 x 1 x *]] }
MPI Rank 0: 0x3b56d08: {[W2*H1 Gradient[132 x 1 x *]] }
MPI Rank 0: 0x3b56ec8: {[B2 Gradient[132 x 1]] }
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: Precomputing --> 3 PreCompute nodes found.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:50: 	MeanOfFeatures = Mean()
MPI Rank 0: 05/03/2016 18:17:50: 	InvStdOfFeatures = InvStdDev()
MPI Rank 0: 05/03/2016 18:17:50: 	Prior = Mean()
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:53: Precomputing --> Completed.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:54: Starting Epoch 1: learning rate per sample = 0.015625  effective momentum = 0.900000  momentum as time constant = 607.4 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:54: Starting minibatch loop.
MPI Rank 0: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: CrossEntropyWithSoftmax = 4.40318406 * 640; EvalErrorPrediction = 0.90468750 * 640; time = 0.1995s; samplesPerSecond = 3207.7
MPI Rank 0: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: CrossEntropyWithSoftmax = 4.15980357 * 640; EvalErrorPrediction = 0.87187500 * 640; time = 0.0954s; samplesPerSecond = 6706.8
MPI Rank 0: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: CrossEntropyWithSoftmax = 3.98424210 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0957s; samplesPerSecond = 6688.9
MPI Rank 0: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: CrossEntropyWithSoftmax = 3.86209050 * 640; EvalErrorPrediction = 0.87656250 * 640; time = 0.0957s; samplesPerSecond = 6684.8
MPI Rank 0: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: CrossEntropyWithSoftmax = 3.80597620 * 640; EvalErrorPrediction = 0.88593750 * 640; time = 0.0958s; samplesPerSecond = 6683.9
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: CrossEntropyWithSoftmax = 3.73511552 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0957s; samplesPerSecond = 6685.0
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: CrossEntropyWithSoftmax = 3.57260725 * 640; EvalErrorPrediction = 0.81875000 * 640; time = 0.0957s; samplesPerSecond = 6688.8
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: CrossEntropyWithSoftmax = 3.42293687 * 640; EvalErrorPrediction = 0.80468750 * 640; time = 0.0958s; samplesPerSecond = 6683.7
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: CrossEntropyWithSoftmax = 3.34304309 * 640; EvalErrorPrediction = 0.76718750 * 640; time = 0.0958s; samplesPerSecond = 6680.2
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: CrossEntropyWithSoftmax = 3.37037793 * 640; EvalErrorPrediction = 0.84687500 * 640; time = 0.0960s; samplesPerSecond = 6666.6
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: CrossEntropyWithSoftmax = 3.21606065 * 640; EvalErrorPrediction = 0.76093750 * 640; time = 0.0954s; samplesPerSecond = 6706.6
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: CrossEntropyWithSoftmax = 3.31610118 * 640; EvalErrorPrediction = 0.78437500 * 640; time = 0.0956s; samplesPerSecond = 6697.2
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: CrossEntropyWithSoftmax = 3.14285888 * 640; EvalErrorPrediction = 0.75000000 * 640; time = 0.0955s; samplesPerSecond = 6702.7
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: CrossEntropyWithSoftmax = 3.01821991 * 640; EvalErrorPrediction = 0.70937500 * 640; time = 0.0950s; samplesPerSecond = 6735.6
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: CrossEntropyWithSoftmax = 3.01218944 * 640; EvalErrorPrediction = 0.73906250 * 640; time = 0.0957s; samplesPerSecond = 6686.8
MPI Rank 0: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: CrossEntropyWithSoftmax = 2.98947652 * 640; EvalErrorPrediction = 0.73593750 * 640; time = 0.0956s; samplesPerSecond = 6691.9
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: CrossEntropyWithSoftmax = 2.86297716 * 640; EvalErrorPrediction = 0.70000000 * 640; time = 0.0957s; samplesPerSecond = 6688.8
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: CrossEntropyWithSoftmax = 2.71901077 * 640; EvalErrorPrediction = 0.68593750 * 640; time = 0.0960s; samplesPerSecond = 6663.5
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: CrossEntropyWithSoftmax = 2.80860596 * 640; EvalErrorPrediction = 0.71250000 * 640; time = 0.0955s; samplesPerSecond = 6702.1
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: CrossEntropyWithSoftmax = 2.60590434 * 640; EvalErrorPrediction = 0.64687500 * 640; time = 0.0953s; samplesPerSecond = 6712.5
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: CrossEntropyWithSoftmax = 2.63920069 * 640; EvalErrorPrediction = 0.66875000 * 640; time = 0.0954s; samplesPerSecond = 6706.9
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: CrossEntropyWithSoftmax = 2.58372597 * 640; EvalErrorPrediction = 0.65781250 * 640; time = 0.0951s; samplesPerSecond = 6731.2
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: CrossEntropyWithSoftmax = 2.50997096 * 640; EvalErrorPrediction = 0.62031250 * 640; time = 0.0957s; samplesPerSecond = 6688.5
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: CrossEntropyWithSoftmax = 2.42126950 * 640; EvalErrorPrediction = 0.62968750 * 640; time = 0.0961s; samplesPerSecond = 6660.2
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: CrossEntropyWithSoftmax = 2.40125789 * 640; EvalErrorPrediction = 0.65156250 * 640; time = 0.0955s; samplesPerSecond = 6704.0
MPI Rank 0: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: CrossEntropyWithSoftmax = 2.47110816 * 640; EvalErrorPrediction = 0.63281250 * 640; time = 0.0954s; samplesPerSecond = 6708.7
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: CrossEntropyWithSoftmax = 2.33215267 * 640; EvalErrorPrediction = 0.60312500 * 640; time = 0.0940s; samplesPerSecond = 6806.6
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: CrossEntropyWithSoftmax = 2.21936103 * 640; EvalErrorPrediction = 0.56875000 * 640; time = 0.0950s; samplesPerSecond = 6739.8
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: CrossEntropyWithSoftmax = 2.31959580 * 640; EvalErrorPrediction = 0.61093750 * 640; time = 0.0950s; samplesPerSecond = 6736.4
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: CrossEntropyWithSoftmax = 2.19592881 * 640; EvalErrorPrediction = 0.61718750 * 640; time = 0.0949s; samplesPerSecond = 6743.7
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: CrossEntropyWithSoftmax = 2.28411654 * 640; EvalErrorPrediction = 0.60000000 * 640; time = 0.0949s; samplesPerSecond = 6743.4
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: CrossEntropyWithSoftmax = 2.18307184 * 640; EvalErrorPrediction = 0.55781250 * 640; time = 0.0720s; samplesPerSecond = 8889.6
MPI Rank 0: 05/03/2016 18:17:57: Finished Epoch[ 1 of 4]: [Training] CrossEntropyWithSoftmax = 2.99723568 * 20480; EvalErrorPrediction = 0.72426758 * 20480; totalSamplesSeen = 20480; learningRatePerSample = 0.015625; epochTime=3.1828s
MPI Rank 0: 05/03/2016 18:17:57: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn.1'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:57: Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.656119  momentum as time constant = 607.5 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:57: Starting minibatch loop, DataParallelSGD training (MyRank = 0, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 0: Actual gradient aggregation time: 0.012992
MPI Rank 0: Async gradient aggregation wait time: 0.004643
MPI Rank 0: Actual gradient aggregation time: 0.007964
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[   1-  10, 12.50%]: CrossEntropyWithSoftmax = 2.08990053 * 2304; EvalErrorPrediction = 0.56640625 * 2304; time = 0.1340s; samplesPerSecond = 17197.5
MPI Rank 0: Async gradient aggregation wait time: 0.010201
MPI Rank 0: Actual gradient aggregation time: 0.015208
MPI Rank 0: Async gradient aggregation wait time: 0.004597
MPI Rank 0: Actual gradient aggregation time: 0.014249
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  11-  20, 25.00%]: CrossEntropyWithSoftmax = 2.16003887 * 2560; EvalErrorPrediction = 0.58476562 * 2560; time = 0.1482s; samplesPerSecond = 17268.5
MPI Rank 0: Async gradient aggregation wait time: 0.004595
MPI Rank 0: Actual gradient aggregation time: 0.012777
MPI Rank 0: Async gradient aggregation wait time: 0.004582
MPI Rank 0: Actual gradient aggregation time: 0.015254
MPI Rank 0: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  21-  30, 37.50%]: CrossEntropyWithSoftmax = 2.19985756 * 2560; EvalErrorPrediction = 0.59492188 * 2560; time = 0.1433s; samplesPerSecond = 17865.7
MPI Rank 0: Async gradient aggregation wait time: 0.004371
MPI Rank 0: Actual gradient aggregation time: 0.01498
MPI Rank 0: Async gradient aggregation wait time: 0.004524
MPI Rank 0: Actual gradient aggregation time: 0.015111
MPI Rank 0: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  31-  40, 50.00%]: CrossEntropyWithSoftmax = 2.12388714 * 2560; EvalErrorPrediction = 0.57968750 * 2560; time = 0.1492s; samplesPerSecond = 17160.5
MPI Rank 0: Async gradient aggregation wait time: 0.005062
MPI Rank 0: Actual gradient aggregation time: 0.015274
MPI Rank 0: Async gradient aggregation wait time: 0.004494
MPI Rank 0: Actual gradient aggregation time: 0.014957
MPI Rank 0: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  41-  50, 62.50%]: CrossEntropyWithSoftmax = 2.05908444 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1524s; samplesPerSecond = 16798.2
MPI Rank 0: Async gradient aggregation wait time: 0.004484
MPI Rank 0: Actual gradient aggregation time: 0.014571
MPI Rank 0: Async gradient aggregation wait time: 0.004407
MPI Rank 0: Actual gradient aggregation time: 0.015128
MPI Rank 0: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  51-  60, 75.00%]: CrossEntropyWithSoftmax = 2.13603725 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1515s; samplesPerSecond = 16892.9
MPI Rank 0: Async gradient aggregation wait time: 0.00506
MPI Rank 0: Actual gradient aggregation time: 0.013164
MPI Rank 0: Async gradient aggregation wait time: 0.004542
MPI Rank 0: Actual gradient aggregation time: 0.015448
MPI Rank 0: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  61-  70, 87.50%]: CrossEntropyWithSoftmax = 2.09094421 * 2560; EvalErrorPrediction = 0.56406250 * 2560; time = 0.1505s; samplesPerSecond = 17014.0
MPI Rank 0: Async gradient aggregation wait time: 0.00451
MPI Rank 0: Actual gradient aggregation time: 0.015246
MPI Rank 0: Async gradient aggregation wait time: 0.004543
MPI Rank 0: Actual gradient aggregation time: 0.015079
MPI Rank 0: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  71-  80, 100.00%]: CrossEntropyWithSoftmax = 2.02829896 * 2560; EvalErrorPrediction = 0.56210938 * 2560; time = 0.1497s; samplesPerSecond = 17097.0
MPI Rank 0: Async gradient aggregation wait time: 0.006283
MPI Rank 0: Actual gradient aggregation time: 0.006489
MPI Rank 0: 05/03/2016 18:17:58: Finished Epoch[ 2 of 4]: [Training] CrossEntropyWithSoftmax = 2.10928672 * 20480; EvalErrorPrediction = 0.57392578 * 20480; totalSamplesSeen = 40960; learningRatePerSample = 0.001953125; epochTime=1.19398s
MPI Rank 0: 05/03/2016 18:17:58: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn.2'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:58: Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:58: Starting minibatch loop, DataParallelSGD training (MyRank = 0, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 0: Async gradient aggregation wait time: 0.00419
MPI Rank 0: Actual gradient aggregation time: 0.02843
MPI Rank 0: Async gradient aggregation wait time: 0.004076
MPI Rank 0: Actual gradient aggregation time: 0.029096
MPI Rank 0: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.99429576 * 9216; EvalErrorPrediction = 0.54709201 * 9216; time = 0.2694s; samplesPerSecond = 34204.1
MPI Rank 0: Async gradient aggregation wait time: 0.002983
MPI Rank 0: Actual gradient aggregation time: 0.028394
MPI Rank 0: Async gradient aggregation wait time: 0.004046
MPI Rank 0: Actual gradient aggregation time: 0.028312
MPI Rank 0: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.92530640 * 10240; EvalErrorPrediction = 0.52812500 * 10240; time = 0.2601s; samplesPerSecond = 39366.0
MPI Rank 0: 05/03/2016 18:17:59: Finished Epoch[ 3 of 4]: [Training] CrossEntropyWithSoftmax = 1.95886100 * 20480; EvalErrorPrediction = 0.53725586 * 20480; totalSamplesSeen = 61440; learningRatePerSample = 9.7656251e-05; epochTime=0.543814s
MPI Rank 0: 05/03/2016 18:17:59: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn.3'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:59: Starting Epoch 4: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:17:59: Starting minibatch loop, DataParallelSGD training (MyRank = 0, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 0: Async gradient aggregation wait time: 0.004651
MPI Rank 0: Actual gradient aggregation time: 0.030131
MPI Rank 0: Async gradient aggregation wait time: 0.006592
MPI Rank 0: Actual gradient aggregation time: 0.024651
MPI Rank 0: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.90011431 * 9216; EvalErrorPrediction = 0.51725260 * 9216; time = 0.2713s; samplesPerSecond = 33974.9
MPI Rank 0: Async gradient aggregation wait time: 0.004322
MPI Rank 0: Actual gradient aggregation time: 0.029603
MPI Rank 0: Async gradient aggregation wait time: 0.006214
MPI Rank 0: Actual gradient aggregation time: 0.023129
MPI Rank 0: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.88429973 * 10240; EvalErrorPrediction = 0.52099609 * 10240; time = 0.2741s; samplesPerSecond = 37364.6
MPI Rank 0: Async gradient aggregation wait time: 0.006246
MPI Rank 0: 05/03/2016 18:17:59: Finished Epoch[ 4 of 4]: [Training] CrossEntropyWithSoftmax = 1.89248911 * 20480; EvalErrorPrediction = 0.51933594 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 9.7656251e-05; epochTime=0.562062s
MPI Rank 0: 05/03/2016 18:17:59: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn'
MPI Rank 0: 05/03/2016 18:18:00: CNTKCommandTrainEnd: speechTrain
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:18:00: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:18:00: __COMPLETED__
MPI Rank 1: 05/03/2016 18:17:50: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 18:17:50: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: 		Built time: May  3 2016 17:56:15
MPI Rank 1: 05/03/2016 18:17:50: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 1: 05/03/2016 18:17:50: 		Build type: release
MPI Rank 1: 05/03/2016 18:17:50: 		Build target: GPU
MPI Rank 1: 05/03/2016 18:17:50: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 18:17:50: 		Math lib: acml
MPI Rank 1: 05/03/2016 18:17:50: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 05/03/2016 18:17:50: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 05/03/2016 18:17:50: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 05/03/2016 18:17:50: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 18:17:50: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 1: 05/03/2016 18:17:50: 		Built by philly on 18750d26eb32
MPI Rank 1: 05/03/2016 18:17:50: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 05/03/2016 18:17:50: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Running on localhost at 2016/05/03 18:17:50
MPI Rank 1: 05/03/2016 18:17:50: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN  OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DeviceId=0  timestamping=true  speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]  speechTrain=[reader=[prefetch=true]]  numCPUThreads=8  precision=double  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]  speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]  speechTrain=[SGD=[maxEpochs=4]]  speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]  stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:17:50: precision = "float"
MPI Rank 1: command = speechTrain
MPI Rank 1: deviceId = $DeviceId$
MPI Rank 1: parallelTrain = true
MPI Rank 1: speechTrain = [
MPI Rank 1:     action = "train"
MPI Rank 1:     modelPath = "$RunDir$/models/cntkSpeech.dnn"
MPI Rank 1:     deviceId = $DeviceId$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SimpleNetworkBuilder = [
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 1:         evalCriterion = "ErrorPrediction"
MPI Rank 1:         layerTypes = "Sigmoid"
MPI Rank 1:         initValueScale = 1.0
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         uniformInit = true
MPI Rank 1:         needPrior = true
MPI Rank 1:     ]
MPI Rank 1:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = 'CE'
MPI Rank 1:         evalCriterion = 'Err'
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 1:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 1:         featNorm = if applyMeanVarNorm
MPI Rank 1:                    then MeanVarNorm(features)
MPI Rank 1:                    else features
MPI Rank 1:         layers[layer:1..L-1] = if layer > 1
MPI Rank 1:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 1:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 1:         CE = if trainingCriterion == 'CE'
MPI Rank 1:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 1:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 1:         Err = if evalCriterion == 'Err' then
MPI Rank 1:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 1:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 1:         logPrior = LogPrior(labels)
MPI Rank 1:         // TODO: how to add a tag to an infix operation?
MPI Rank 1:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 1:     ]
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize = 20480
MPI Rank 1:         minibatchSize = 64:256:1024
MPI Rank 1:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 1:         numMBsToShowResult = 10
MPI Rank 1:         momentumPerMB = 0.9:0.656119
MPI Rank 1:         dropoutRate = 0.0
MPI Rank 1:         maxEpochs = 3
MPI Rank 1:         keepCheckPointFiles = true
MPI Rank 1:         clippingThresholdPerSample = 1#INF
MPI Rank 1:         ParallelTrain = [
MPI Rank 1:             parallelizationMethod = "DataParallelSGD"
MPI Rank 1:             distributedMBReading = true
MPI Rank 1:             DataParallelSGD = [
MPI Rank 1:                 gradientBits = 32
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1:         AutoAdjust = [
MPI Rank 1:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 1:             loadBestModel = true
MPI Rank 1:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 1:             learnRateDecreaseFactor = 0.5
MPI Rank 1:             learnRateIncreaseFactor = 1.382
MPI Rank 1:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1:     reader = [
MPI Rank 1:         readerType = "HTKMLFReader"
MPI Rank 1:         readMethod = "blockRandomize"
MPI Rank 1:         miniBatchMode = "partial"
MPI Rank 1:         randomize = "auto"
MPI Rank 1:         verbosity = 0
MPI Rank 1:         features = [
MPI Rank 1:             dim = 363
MPI Rank 1:             type = "real"
MPI Rank 1:             scpFile = "glob_0000.scp"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             mlfFile = "$DataDir$/glob_0000.mlf"
MPI Rank 1:             labelMappingFile = "$DataDir$/state.list"
MPI Rank 1:             labelDim = 132
MPI Rank 1:             labelType = "category"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 1: speechTrain=[reader=[prefetch=true]]
MPI Rank 1: numCPUThreads=8
MPI Rank 1: precision=double
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 1: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:17:50: precision = "float"
MPI Rank 1: command = speechTrain
MPI Rank 1: deviceId = 0
MPI Rank 1: parallelTrain = true
MPI Rank 1: speechTrain = [
MPI Rank 1:     action = "train"
MPI Rank 1:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 1:     deviceId = 0
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SimpleNetworkBuilder = [
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 1:         evalCriterion = "ErrorPrediction"
MPI Rank 1:         layerTypes = "Sigmoid"
MPI Rank 1:         initValueScale = 1.0
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         uniformInit = true
MPI Rank 1:         needPrior = true
MPI Rank 1:     ]
MPI Rank 1:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = 'CE'
MPI Rank 1:         evalCriterion = 'Err'
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 1:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 1:         featNorm = if applyMeanVarNorm
MPI Rank 1:                    then MeanVarNorm(features)
MPI Rank 1:                    else features
MPI Rank 1:         layers[layer:1..L-1] = if layer > 1
MPI Rank 1:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 1:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 1:         CE = if trainingCriterion == 'CE'
MPI Rank 1:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 1:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 1:         Err = if evalCriterion == 'Err' then
MPI Rank 1:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 1:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 1:         logPrior = LogPrior(labels)
MPI Rank 1:         // TODO: how to add a tag to an infix operation?
MPI Rank 1:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 1:     ]
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize = 20480
MPI Rank 1:         minibatchSize = 64:256:1024
MPI Rank 1:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 1:         numMBsToShowResult = 10
MPI Rank 1:         momentumPerMB = 0.9:0.656119
MPI Rank 1:         dropoutRate = 0.0
MPI Rank 1:         maxEpochs = 3
MPI Rank 1:         keepCheckPointFiles = true
MPI Rank 1:         clippingThresholdPerSample = 1#INF
MPI Rank 1:         ParallelTrain = [
MPI Rank 1:             parallelizationMethod = "DataParallelSGD"
MPI Rank 1:             distributedMBReading = true
MPI Rank 1:             DataParallelSGD = [
MPI Rank 1:                 gradientBits = 32
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1:         AutoAdjust = [
MPI Rank 1:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 1:             loadBestModel = true
MPI Rank 1:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 1:             learnRateDecreaseFactor = 0.5
MPI Rank 1:             learnRateIncreaseFactor = 1.382
MPI Rank 1:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1:     reader = [
MPI Rank 1:         readerType = "HTKMLFReader"
MPI Rank 1:         readMethod = "blockRandomize"
MPI Rank 1:         miniBatchMode = "partial"
MPI Rank 1:         randomize = "auto"
MPI Rank 1:         verbosity = 0
MPI Rank 1:         features = [
MPI Rank 1:             dim = 363
MPI Rank 1:             type = "real"
MPI Rank 1:             scpFile = "glob_0000.scp"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 1:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 1:             labelDim = 132
MPI Rank 1:             labelType = "category"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 1: speechTrain=[reader=[prefetch=true]]
MPI Rank 1: numCPUThreads=8
MPI Rank 1: precision=double
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 1: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 1: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: cntk.cntk:command=speechTrain
MPI Rank 1: configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 1: configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 1: configparameters: cntk.cntk:deviceId=0
MPI Rank 1: configparameters: cntk.cntk:numCPUThreads=8
MPI Rank 1: configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: configparameters: cntk.cntk:parallelTrain=true
MPI Rank 1: configparameters: cntk.cntk:precision=double
MPI Rank 1: configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 1: configparameters: cntk.cntk:speechTrain=[
MPI Rank 1:     action = "train"
MPI Rank 1:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 1:     deviceId = 0
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SimpleNetworkBuilder = [
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 1:         evalCriterion = "ErrorPrediction"
MPI Rank 1:         layerTypes = "Sigmoid"
MPI Rank 1:         initValueScale = 1.0
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         uniformInit = true
MPI Rank 1:         needPrior = true
MPI Rank 1:     ]
MPI Rank 1:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 1:         layerSizes = 363:512:512:132
MPI Rank 1:         trainingCriterion = 'CE'
MPI Rank 1:         evalCriterion = 'Err'
MPI Rank 1:         applyMeanVarNorm = true
MPI Rank 1:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 1:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 1:         featNorm = if applyMeanVarNorm
MPI Rank 1:                    then MeanVarNorm(features)
MPI Rank 1:                    else features
MPI Rank 1:         layers[layer:1..L-1] = if layer > 1
MPI Rank 1:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 1:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 1:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 1:         CE = if trainingCriterion == 'CE'
MPI Rank 1:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 1:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 1:         Err = if evalCriterion == 'Err' then
MPI Rank 1:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 1:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 1:         logPrior = LogPrior(labels)
MPI Rank 1:         // TODO: how to add a tag to an infix operation?
MPI Rank 1:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 1:     ]
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize = 20480
MPI Rank 1:         minibatchSize = 64:256:1024
MPI Rank 1:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 1:         numMBsToShowResult = 10
MPI Rank 1:         momentumPerMB = 0.9:0.656119
MPI Rank 1:         dropoutRate = 0.0
MPI Rank 1:         maxEpochs = 3
MPI Rank 1:         keepCheckPointFiles = true
MPI Rank 1:         clippingThresholdPerSample = 1#INF
MPI Rank 1:         ParallelTrain = [
MPI Rank 1:             parallelizationMethod = "DataParallelSGD"
MPI Rank 1:             distributedMBReading = true
MPI Rank 1:             DataParallelSGD = [
MPI Rank 1:                 gradientBits = 32
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1:         AutoAdjust = [
MPI Rank 1:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 1:             loadBestModel = true
MPI Rank 1:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 1:             learnRateDecreaseFactor = 0.5
MPI Rank 1:             learnRateIncreaseFactor = 1.382
MPI Rank 1:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1:     reader = [
MPI Rank 1:         readerType = "HTKMLFReader"
MPI Rank 1:         readMethod = "blockRandomize"
MPI Rank 1:         miniBatchMode = "partial"
MPI Rank 1:         randomize = "auto"
MPI Rank 1:         verbosity = 0
MPI Rank 1:         features = [
MPI Rank 1:             dim = 363
MPI Rank 1:             type = "real"
MPI Rank 1:             scpFile = "glob_0000.scp"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 1:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 1:             labelDim = 132
MPI Rank 1:             labelType = "category"
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1: ] [reader=[readerType=ExperimentalHTKMLFReader]] [reader=[prefetch=true]] [SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]] [SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]] [SGD=[ParallelTrain=[parallelizationStartEpoch=2]]] [SGD=[maxEpochs=4]] [SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 1: 
MPI Rank 1: configparameters: cntk.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 1: configparameters: cntk.cntk:timestamping=true
MPI Rank 1: 05/03/2016 18:17:50: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 18:17:50: Commands: speechTrain
MPI Rank 1: 05/03/2016 18:17:50: Precision = "double"
MPI Rank 1: 05/03/2016 18:17:50: Using 8 CPU threads.
MPI Rank 1: 05/03/2016 18:17:50: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn
MPI Rank 1: 05/03/2016 18:17:50: CNTKCommandTrainInfo: speechTrain : 4
MPI Rank 1: 05/03/2016 18:17:50: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 4
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: ##############################################################################
MPI Rank 1: 05/03/2016 18:17:50: #                                                                            #
MPI Rank 1: 05/03/2016 18:17:50: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 18:17:50: #                                                                            #
MPI Rank 1: 05/03/2016 18:17:50: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: CNTKCommandTrainBegin: speechTrain
MPI Rank 1: SimpleNetworkBuilder Using GPU 0
MPI Rank 1: Reading script file glob_0000.scp ... 948 entries
MPI Rank 1: HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 1: HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 1: total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
MPI Rank 1: htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MPI Rank 1: MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MPI Rank 1: MLFDataDeserializer::MLFDataDeserializer: read 948 utterances
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Creating virgin network.
MPI Rank 1: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==8
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 7 roots:
MPI Rank 1: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
MPI Rank 1: 	EvalErrorPrediction = ErrorPrediction()
MPI Rank 1: 	InvStdOfFeatures = InvStdDev()
MPI Rank 1: 	MeanOfFeatures = Mean()
MPI Rank 1: 	PosteriorProb = Softmax()
MPI Rank 1: 	Prior = Mean()
MPI Rank 1: 	ScaledLogLikelihood = Minus()
MPI Rank 1: 
MPI Rank 1: Validating network. 25 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> labels = InputValue() :  -> [132 x *]
MPI Rank 1: Validating --> W2 = LearnableParameter() :  -> [132 x 512]
MPI Rank 1: Validating --> W1 = LearnableParameter() :  -> [512 x 512]
MPI Rank 1: Validating --> W0 = LearnableParameter() :  -> [512 x 363]
MPI Rank 1: Validating --> features = InputValue() :  -> [363 x *]
MPI Rank 1: Validating --> MeanOfFeatures = Mean (features) : [363 x *] -> [363]
MPI Rank 1: Validating --> InvStdOfFeatures = InvStdDev (features) : [363 x *] -> [363]
MPI Rank 1: Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [363 x *], [363], [363] -> [363 x *]
MPI Rank 1: Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [512 x 363], [363 x *] -> [512 x *]
MPI Rank 1: Validating --> B0 = LearnableParameter() :  -> [512 x 1]
MPI Rank 1: Validating --> W0*features+B0 = Plus (W0*features, B0) : [512 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 1: Validating --> H1 = Sigmoid (W0*features+B0) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> W1*H1 = Times (W1, H1) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> B1 = LearnableParameter() :  -> [512 x 1]
MPI Rank 1: Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 1: Validating --> H2 = Sigmoid (W1*H1+B1) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> W2*H1 = Times (W2, H2) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
MPI Rank 1: Validating --> B2 = LearnableParameter() :  -> [132 x 1]
MPI Rank 1: Validating --> HLast = Plus (W2*H1, B2) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
MPI Rank 1: Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 1: Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 1: Validating --> PosteriorProb = Softmax (HLast) : [132 x 1 x *] -> [132 x 1 x *]
MPI Rank 1: Validating --> Prior = Mean (labels) : [132 x *] -> [132]
MPI Rank 1: Validating --> LogOfPrior = Log (Prior) : [132] -> [132]
MPI Rank 1: Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [132 x 1 x *], [132] -> [132 x 1 x *]
MPI Rank 1: 
MPI Rank 1: Validating network. 17 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 12 out of 25 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Created model with 25 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Training criterion node(s):
MPI Rank 1: 05/03/2016 18:17:50: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Evaluation criterion node(s):
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: 	EvalErrorPrediction = ErrorPrediction
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[EvalErrorPrediction Gradient[1]] [InvStdOfFeatures Gradient[363]] [LogOfPrior Gradient[132]] [MVNormalizedFeatures Gradient[363 x *]] [MeanOfFeatures Gradient[363]] [PosteriorProb Gradient[132 x 1 x *]] [PosteriorProb Value[132 x 1 x *]] [Prior Gradient[132]] [ScaledLogLikelihood Gradient[132 x 1 x *]] [features Gradient[363 x *]] [labels Gradient[132 x *]] }
MPI Rank 1: 0x2654e68: {[features Value[363 x *]] }
MPI Rank 1: 0x2ebc048: {[MeanOfFeatures Value[363]] }
MPI Rank 1: 0x2ebd208: {[InvStdOfFeatures Value[363]] }
MPI Rank 1: 0x2ebdf38: {[W0 Value[512 x 363]] }
MPI Rank 1: 0x33e01f8: {[W1 Value[512 x 512]] }
MPI Rank 1: 0x33e0fc8: {[B1 Value[512 x 1]] }
MPI Rank 1: 0x33e2168: {[W2 Value[132 x 512]] }
MPI Rank 1: 0x33e2e18: {[B2 Value[132 x 1]] }
MPI Rank 1: 0x33e3c48: {[labels Value[132 x *]] }
MPI Rank 1: 0x33e4ea8: {[Prior Value[132]] }
MPI Rank 1: 0x33ea748: {[EvalErrorPrediction Value[1]] }
MPI Rank 1: 0x33eaa48: {[ScaledLogLikelihood Value[132 x 1 x *]] }
MPI Rank 1: 0x33eac08: {[CrossEntropyWithSoftmax Value[1]] }
MPI Rank 1: 0x33eb098: {[W0 Gradient[512 x 363]] [W0*features+B0 Value[512 x 1 x *]] }
MPI Rank 1: 0x33eb208: {[LogOfPrior Value[132]] }
MPI Rank 1: 0x33f0808: {[B0 Value[512 x 1]] }
MPI Rank 1: 0x36cbed8: {[MVNormalizedFeatures Value[363 x *]] }
MPI Rank 1: 0x36cc698: {[W0*features Value[512 x *]] }
MPI Rank 1: 0x36cc8a8: {[H1 Value[512 x 1 x *]] [W0*features Gradient[512 x *]] }
MPI Rank 1: 0x36cca08: {[W0*features+B0 Gradient[512 x 1 x *]] [W1*H1 Value[512 x 1 x *]] }
MPI Rank 1: 0x36ccb68: {[W1 Gradient[512 x 512]] [W1*H1+B1 Value[512 x 1 x *]] }
MPI Rank 1: 0x36ccd28: {[H2 Value[512 x 1 x *]] [W1*H1 Gradient[512 x 1 x *]] }
MPI Rank 1: 0x36ccee8: {[B0 Gradient[512 x 1]] [H1 Gradient[512 x 1 x *]] [W1*H1+B1 Gradient[512 x 1 x *]] [W2*H1 Value[132 x 1 x *]] }
MPI Rank 1: 0x36cd0a8: {[HLast Value[132 x 1 x *]] [W2 Gradient[132 x 512]] }
MPI Rank 1: 0x36cdc08: {[CrossEntropyWithSoftmax Gradient[1]] }
MPI Rank 1: 0x36cddc8: {[B1 Gradient[512 x 1]] [H2 Gradient[512 x 1 x *]] [HLast Gradient[132 x 1 x *]] }
MPI Rank 1: 0x36cdf88: {[W2*H1 Gradient[132 x 1 x *]] }
MPI Rank 1: 0x36ce148: {[B2 Gradient[132 x 1]] }
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: Precomputing --> 3 PreCompute nodes found.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:50: 	MeanOfFeatures = Mean()
MPI Rank 1: 05/03/2016 18:17:50: 	InvStdOfFeatures = InvStdDev()
MPI Rank 1: 05/03/2016 18:17:50: 	Prior = Mean()
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:53: Precomputing --> Completed.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:54: Starting Epoch 1: learning rate per sample = 0.015625  effective momentum = 0.900000  momentum as time constant = 607.4 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:54: Starting minibatch loop.
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: CrossEntropyWithSoftmax = 4.40318406 * 640; EvalErrorPrediction = 0.90468750 * 640; time = 0.1373s; samplesPerSecond = 4662.7
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: CrossEntropyWithSoftmax = 4.15980357 * 640; EvalErrorPrediction = 0.87187500 * 640; time = 0.0955s; samplesPerSecond = 6702.9
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: CrossEntropyWithSoftmax = 3.98424210 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0955s; samplesPerSecond = 6704.2
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: CrossEntropyWithSoftmax = 3.86209050 * 640; EvalErrorPrediction = 0.87656250 * 640; time = 0.0957s; samplesPerSecond = 6685.5
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: CrossEntropyWithSoftmax = 3.80597620 * 640; EvalErrorPrediction = 0.88593750 * 640; time = 0.0958s; samplesPerSecond = 6681.9
MPI Rank 1: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: CrossEntropyWithSoftmax = 3.73511552 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0957s; samplesPerSecond = 6685.5
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: CrossEntropyWithSoftmax = 3.57260725 * 640; EvalErrorPrediction = 0.81875000 * 640; time = 0.0957s; samplesPerSecond = 6684.7
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: CrossEntropyWithSoftmax = 3.42293687 * 640; EvalErrorPrediction = 0.80468750 * 640; time = 0.0957s; samplesPerSecond = 6690.0
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: CrossEntropyWithSoftmax = 3.34304309 * 640; EvalErrorPrediction = 0.76718750 * 640; time = 0.0958s; samplesPerSecond = 6677.2
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: CrossEntropyWithSoftmax = 3.37037793 * 640; EvalErrorPrediction = 0.84687500 * 640; time = 0.0957s; samplesPerSecond = 6690.9
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: CrossEntropyWithSoftmax = 3.21606065 * 640; EvalErrorPrediction = 0.76093750 * 640; time = 0.0954s; samplesPerSecond = 6709.6
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: CrossEntropyWithSoftmax = 3.31610118 * 640; EvalErrorPrediction = 0.78437500 * 640; time = 0.0955s; samplesPerSecond = 6701.1
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: CrossEntropyWithSoftmax = 3.14285888 * 640; EvalErrorPrediction = 0.75000000 * 640; time = 0.0956s; samplesPerSecond = 6697.6
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: CrossEntropyWithSoftmax = 3.01821991 * 640; EvalErrorPrediction = 0.70937500 * 640; time = 0.0955s; samplesPerSecond = 6702.9
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: CrossEntropyWithSoftmax = 3.01218944 * 640; EvalErrorPrediction = 0.73906250 * 640; time = 0.0956s; samplesPerSecond = 6693.2
MPI Rank 1: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: CrossEntropyWithSoftmax = 2.98947652 * 640; EvalErrorPrediction = 0.73593750 * 640; time = 0.0957s; samplesPerSecond = 6684.8
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: CrossEntropyWithSoftmax = 2.86297716 * 640; EvalErrorPrediction = 0.70000000 * 640; time = 0.0956s; samplesPerSecond = 6695.5
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: CrossEntropyWithSoftmax = 2.71901077 * 640; EvalErrorPrediction = 0.68593750 * 640; time = 0.0957s; samplesPerSecond = 6687.2
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: CrossEntropyWithSoftmax = 2.80860596 * 640; EvalErrorPrediction = 0.71250000 * 640; time = 0.0955s; samplesPerSecond = 6704.4
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: CrossEntropyWithSoftmax = 2.60590434 * 640; EvalErrorPrediction = 0.64687500 * 640; time = 0.0954s; samplesPerSecond = 6708.5
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: CrossEntropyWithSoftmax = 2.63920069 * 640; EvalErrorPrediction = 0.66875000 * 640; time = 0.0954s; samplesPerSecond = 6710.1
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: CrossEntropyWithSoftmax = 2.58372597 * 640; EvalErrorPrediction = 0.65781250 * 640; time = 0.0954s; samplesPerSecond = 6708.8
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: CrossEntropyWithSoftmax = 2.50997096 * 640; EvalErrorPrediction = 0.62031250 * 640; time = 0.0957s; samplesPerSecond = 6685.3
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: CrossEntropyWithSoftmax = 2.42126950 * 640; EvalErrorPrediction = 0.62968750 * 640; time = 0.0957s; samplesPerSecond = 6687.7
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: CrossEntropyWithSoftmax = 2.40125789 * 640; EvalErrorPrediction = 0.65156250 * 640; time = 0.0955s; samplesPerSecond = 6700.7
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: CrossEntropyWithSoftmax = 2.47110816 * 640; EvalErrorPrediction = 0.63281250 * 640; time = 0.0954s; samplesPerSecond = 6705.9
MPI Rank 1: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: CrossEntropyWithSoftmax = 2.33215267 * 640; EvalErrorPrediction = 0.60312500 * 640; time = 0.0978s; samplesPerSecond = 6545.5
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: CrossEntropyWithSoftmax = 2.21936103 * 640; EvalErrorPrediction = 0.56875000 * 640; time = 0.0950s; samplesPerSecond = 6739.2
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: CrossEntropyWithSoftmax = 2.31959580 * 640; EvalErrorPrediction = 0.61093750 * 640; time = 0.0950s; samplesPerSecond = 6737.7
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: CrossEntropyWithSoftmax = 2.19592881 * 640; EvalErrorPrediction = 0.61718750 * 640; time = 0.0950s; samplesPerSecond = 6738.9
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: CrossEntropyWithSoftmax = 2.28411654 * 640; EvalErrorPrediction = 0.60000000 * 640; time = 0.0949s; samplesPerSecond = 6744.6
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: CrossEntropyWithSoftmax = 2.18307184 * 640; EvalErrorPrediction = 0.55781250 * 640; time = 0.0950s; samplesPerSecond = 6739.0
MPI Rank 1: 05/03/2016 18:17:57: Finished Epoch[ 1 of 4]: [Training] CrossEntropyWithSoftmax = 2.99723568 * 20480; EvalErrorPrediction = 0.72426758 * 20480; totalSamplesSeen = 20480; learningRatePerSample = 0.015625; epochTime=3.14345s
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:57: Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.656119  momentum as time constant = 607.5 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:57: Starting minibatch loop, DataParallelSGD training (MyRank = 1, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 1: Actual gradient aggregation time: 0.013894
MPI Rank 1: Async gradient aggregation wait time: 0.004456
MPI Rank 1: Actual gradient aggregation time: 0.009517
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[   1-  10, 12.50%]: CrossEntropyWithSoftmax = 2.08990053 * 2304; EvalErrorPrediction = 0.56640625 * 2304; time = 0.1436s; samplesPerSecond = 16043.8
MPI Rank 1: Async gradient aggregation wait time: 0.003536
MPI Rank 1: Actual gradient aggregation time: 0.015354
MPI Rank 1: Async gradient aggregation wait time: 0.003722
MPI Rank 1: Actual gradient aggregation time: 0.014315
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  11-  20, 25.00%]: CrossEntropyWithSoftmax = 2.16003887 * 2560; EvalErrorPrediction = 0.58476562 * 2560; time = 0.1381s; samplesPerSecond = 18540.2
MPI Rank 1: Async gradient aggregation wait time: 0.003685
MPI Rank 1: Actual gradient aggregation time: 0.013215
MPI Rank 1: Async gradient aggregation wait time: 0.003692
MPI Rank 1: Actual gradient aggregation time: 0.015446
MPI Rank 1: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  21-  30, 37.50%]: CrossEntropyWithSoftmax = 2.19985756 * 2560; EvalErrorPrediction = 0.59492188 * 2560; time = 0.1433s; samplesPerSecond = 17865.4
MPI Rank 1: Async gradient aggregation wait time: 0.005056
MPI Rank 1: Actual gradient aggregation time: 0.014899
MPI Rank 1: Async gradient aggregation wait time: 0.003638
MPI Rank 1: Actual gradient aggregation time: 0.015239
MPI Rank 1: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  31-  40, 50.00%]: CrossEntropyWithSoftmax = 2.12388714 * 2560; EvalErrorPrediction = 0.57968750 * 2560; time = 0.1491s; samplesPerSecond = 17173.4
MPI Rank 1: Async gradient aggregation wait time: 0.003709
MPI Rank 1: Actual gradient aggregation time: 0.015194
MPI Rank 1: Async gradient aggregation wait time: 0.003629
MPI Rank 1: Actual gradient aggregation time: 0.015065
MPI Rank 1: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  41-  50, 62.50%]: CrossEntropyWithSoftmax = 2.05908444 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1525s; samplesPerSecond = 16790.1
MPI Rank 1: Async gradient aggregation wait time: 0.003577
MPI Rank 1: Actual gradient aggregation time: 0.014749
MPI Rank 1: Async gradient aggregation wait time: 0.00337
MPI Rank 1: Actual gradient aggregation time: 0.015318
MPI Rank 1: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  51-  60, 75.00%]: CrossEntropyWithSoftmax = 2.13603725 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1515s; samplesPerSecond = 16895.1
MPI Rank 1: Async gradient aggregation wait time: 0.004412
MPI Rank 1: Actual gradient aggregation time: 0.014349
MPI Rank 1: Async gradient aggregation wait time: 0.003659
MPI Rank 1: Actual gradient aggregation time: 0.01547
MPI Rank 1: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  61-  70, 87.50%]: CrossEntropyWithSoftmax = 2.09094421 * 2560; EvalErrorPrediction = 0.56406250 * 2560; time = 0.1505s; samplesPerSecond = 17006.0
MPI Rank 1: Async gradient aggregation wait time: 0.003496
MPI Rank 1: Actual gradient aggregation time: 0.015411
MPI Rank 1: Async gradient aggregation wait time: 0.003639
MPI Rank 1: Actual gradient aggregation time: 0.015077
MPI Rank 1: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  71-  80, 100.00%]: CrossEntropyWithSoftmax = 2.02829896 * 2560; EvalErrorPrediction = 0.56210938 * 2560; time = 0.1495s; samplesPerSecond = 17119.2
MPI Rank 1: Async gradient aggregation wait time: 0.006459
MPI Rank 1: Actual gradient aggregation time: 0.006551
MPI Rank 1: 05/03/2016 18:17:58: Finished Epoch[ 2 of 4]: [Training] CrossEntropyWithSoftmax = 2.10928672 * 20480; EvalErrorPrediction = 0.57392578 * 20480; totalSamplesSeen = 40960; learningRatePerSample = 0.001953125; epochTime=1.19347s
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:58: Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:58: Starting minibatch loop, DataParallelSGD training (MyRank = 1, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 1: Async gradient aggregation wait time: 0.013751
MPI Rank 1: Actual gradient aggregation time: 0.028508
MPI Rank 1: Async gradient aggregation wait time: 0.003459
MPI Rank 1: Actual gradient aggregation time: 0.029236
MPI Rank 1: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.99429576 * 9216; EvalErrorPrediction = 0.54709201 * 9216; time = 0.2689s; samplesPerSecond = 34270.2
MPI Rank 1: Async gradient aggregation wait time: 0.002465
MPI Rank 1: Actual gradient aggregation time: 0.028459
MPI Rank 1: Async gradient aggregation wait time: 0.000286
MPI Rank 1: Actual gradient aggregation time: 0.015157
MPI Rank 1: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.92530640 * 10240; EvalErrorPrediction = 0.52812500 * 10240; time = 0.2555s; samplesPerSecond = 40080.6
MPI Rank 1: 05/03/2016 18:17:59: Finished Epoch[ 3 of 4]: [Training] CrossEntropyWithSoftmax = 1.95886100 * 20480; EvalErrorPrediction = 0.53725586 * 20480; totalSamplesSeen = 61440; learningRatePerSample = 9.7656251e-05; epochTime=0.543309s
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:59: Starting Epoch 4: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:17:59: Starting minibatch loop, DataParallelSGD training (MyRank = 1, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 1: Async gradient aggregation wait time: 0.004189
MPI Rank 1: Actual gradient aggregation time: 0.03077
MPI Rank 1: Async gradient aggregation wait time: 0.003386
MPI Rank 1: Actual gradient aggregation time: 0.02757
MPI Rank 1: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.90011431 * 9216; EvalErrorPrediction = 0.51725260 * 9216; time = 0.2711s; samplesPerSecond = 33998.6
MPI Rank 1: Async gradient aggregation wait time: 0.003756
MPI Rank 1: Actual gradient aggregation time: 0.029704
MPI Rank 1: Async gradient aggregation wait time: 0.000726
MPI Rank 1: Actual gradient aggregation time: 0.023982
MPI Rank 1: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.88429973 * 10240; EvalErrorPrediction = 0.52099609 * 10240; time = 0.2741s; samplesPerSecond = 37354.4
MPI Rank 1: Async gradient aggregation wait time: 0.006279
MPI Rank 1: 05/03/2016 18:17:59: Finished Epoch[ 4 of 4]: [Training] CrossEntropyWithSoftmax = 1.89248911 * 20480; EvalErrorPrediction = 0.51933594 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 9.7656251e-05; epochTime=0.561545s
MPI Rank 1: 05/03/2016 18:18:00: CNTKCommandTrainEnd: speechTrain
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:18:00: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:18:00: __COMPLETED__
MPI Rank 2: 05/03/2016 18:17:51: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 18:17:51: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: 		Built time: May  3 2016 17:56:15
MPI Rank 2: 05/03/2016 18:17:51: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 2: 05/03/2016 18:17:51: 		Build type: release
MPI Rank 2: 05/03/2016 18:17:51: 		Build target: GPU
MPI Rank 2: 05/03/2016 18:17:51: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 18:17:51: 		Math lib: acml
MPI Rank 2: 05/03/2016 18:17:51: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 05/03/2016 18:17:51: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 05/03/2016 18:17:51: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 05/03/2016 18:17:51: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 18:17:51: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 2: 05/03/2016 18:17:51: 		Built by philly on 18750d26eb32
MPI Rank 2: 05/03/2016 18:17:51: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 05/03/2016 18:17:51: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Running on localhost at 2016/05/03 18:17:51
MPI Rank 2: 05/03/2016 18:17:51: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN  OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu  DeviceId=0  timestamping=true  speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]  speechTrain=[reader=[prefetch=true]]  numCPUThreads=8  precision=double  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]  speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]  speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]  speechTrain=[SGD=[maxEpochs=4]]  speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]  stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:17:51: precision = "float"
MPI Rank 2: command = speechTrain
MPI Rank 2: deviceId = $DeviceId$
MPI Rank 2: parallelTrain = true
MPI Rank 2: speechTrain = [
MPI Rank 2:     action = "train"
MPI Rank 2:     modelPath = "$RunDir$/models/cntkSpeech.dnn"
MPI Rank 2:     deviceId = $DeviceId$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SimpleNetworkBuilder = [
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 2:         evalCriterion = "ErrorPrediction"
MPI Rank 2:         layerTypes = "Sigmoid"
MPI Rank 2:         initValueScale = 1.0
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         uniformInit = true
MPI Rank 2:         needPrior = true
MPI Rank 2:     ]
MPI Rank 2:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = 'CE'
MPI Rank 2:         evalCriterion = 'Err'
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 2:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 2:         featNorm = if applyMeanVarNorm
MPI Rank 2:                    then MeanVarNorm(features)
MPI Rank 2:                    else features
MPI Rank 2:         layers[layer:1..L-1] = if layer > 1
MPI Rank 2:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 2:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 2:         CE = if trainingCriterion == 'CE'
MPI Rank 2:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 2:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 2:         Err = if evalCriterion == 'Err' then
MPI Rank 2:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 2:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 2:         logPrior = LogPrior(labels)
MPI Rank 2:         // TODO: how to add a tag to an infix operation?
MPI Rank 2:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 2:     ]
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize = 20480
MPI Rank 2:         minibatchSize = 64:256:1024
MPI Rank 2:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 2:         numMBsToShowResult = 10
MPI Rank 2:         momentumPerMB = 0.9:0.656119
MPI Rank 2:         dropoutRate = 0.0
MPI Rank 2:         maxEpochs = 3
MPI Rank 2:         keepCheckPointFiles = true
MPI Rank 2:         clippingThresholdPerSample = 1#INF
MPI Rank 2:         ParallelTrain = [
MPI Rank 2:             parallelizationMethod = "DataParallelSGD"
MPI Rank 2:             distributedMBReading = true
MPI Rank 2:             DataParallelSGD = [
MPI Rank 2:                 gradientBits = 32
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2:         AutoAdjust = [
MPI Rank 2:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 2:             loadBestModel = true
MPI Rank 2:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 2:             learnRateDecreaseFactor = 0.5
MPI Rank 2:             learnRateIncreaseFactor = 1.382
MPI Rank 2:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2:     reader = [
MPI Rank 2:         readerType = "HTKMLFReader"
MPI Rank 2:         readMethod = "blockRandomize"
MPI Rank 2:         miniBatchMode = "partial"
MPI Rank 2:         randomize = "auto"
MPI Rank 2:         verbosity = 0
MPI Rank 2:         features = [
MPI Rank 2:             dim = 363
MPI Rank 2:             type = "real"
MPI Rank 2:             scpFile = "glob_0000.scp"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             mlfFile = "$DataDir$/glob_0000.mlf"
MPI Rank 2:             labelMappingFile = "$DataDir$/state.list"
MPI Rank 2:             labelDim = 132
MPI Rank 2:             labelType = "category"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 2: speechTrain=[reader=[prefetch=true]]
MPI Rank 2: numCPUThreads=8
MPI Rank 2: precision=double
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 2: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:17:51: precision = "float"
MPI Rank 2: command = speechTrain
MPI Rank 2: deviceId = 0
MPI Rank 2: parallelTrain = true
MPI Rank 2: speechTrain = [
MPI Rank 2:     action = "train"
MPI Rank 2:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 2:     deviceId = 0
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SimpleNetworkBuilder = [
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 2:         evalCriterion = "ErrorPrediction"
MPI Rank 2:         layerTypes = "Sigmoid"
MPI Rank 2:         initValueScale = 1.0
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         uniformInit = true
MPI Rank 2:         needPrior = true
MPI Rank 2:     ]
MPI Rank 2:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = 'CE'
MPI Rank 2:         evalCriterion = 'Err'
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 2:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 2:         featNorm = if applyMeanVarNorm
MPI Rank 2:                    then MeanVarNorm(features)
MPI Rank 2:                    else features
MPI Rank 2:         layers[layer:1..L-1] = if layer > 1
MPI Rank 2:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 2:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 2:         CE = if trainingCriterion == 'CE'
MPI Rank 2:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 2:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 2:         Err = if evalCriterion == 'Err' then
MPI Rank 2:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 2:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 2:         logPrior = LogPrior(labels)
MPI Rank 2:         // TODO: how to add a tag to an infix operation?
MPI Rank 2:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 2:     ]
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize = 20480
MPI Rank 2:         minibatchSize = 64:256:1024
MPI Rank 2:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 2:         numMBsToShowResult = 10
MPI Rank 2:         momentumPerMB = 0.9:0.656119
MPI Rank 2:         dropoutRate = 0.0
MPI Rank 2:         maxEpochs = 3
MPI Rank 2:         keepCheckPointFiles = true
MPI Rank 2:         clippingThresholdPerSample = 1#INF
MPI Rank 2:         ParallelTrain = [
MPI Rank 2:             parallelizationMethod = "DataParallelSGD"
MPI Rank 2:             distributedMBReading = true
MPI Rank 2:             DataParallelSGD = [
MPI Rank 2:                 gradientBits = 32
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2:         AutoAdjust = [
MPI Rank 2:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 2:             loadBestModel = true
MPI Rank 2:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 2:             learnRateDecreaseFactor = 0.5
MPI Rank 2:             learnRateIncreaseFactor = 1.382
MPI Rank 2:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2:     reader = [
MPI Rank 2:         readerType = "HTKMLFReader"
MPI Rank 2:         readMethod = "blockRandomize"
MPI Rank 2:         miniBatchMode = "partial"
MPI Rank 2:         randomize = "auto"
MPI Rank 2:         verbosity = 0
MPI Rank 2:         features = [
MPI Rank 2:             dim = 363
MPI Rank 2:             type = "real"
MPI Rank 2:             scpFile = "glob_0000.scp"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 2:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 2:             labelDim = 132
MPI Rank 2:             labelType = "category"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: speechTrain=[reader=[readerType=ExperimentalHTKMLFReader]]
MPI Rank 2: speechTrain=[reader=[prefetch=true]]
MPI Rank 2: numCPUThreads=8
MPI Rank 2: precision=double
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[parallelizationStartEpoch=2]]]
MPI Rank 2: speechTrain=[SGD=[maxEpochs=4]]
MPI Rank 2: speechTrain=[SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: cntk.cntk:command=speechTrain
MPI Rank 2: configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/ExperimentalHtkmlfReader/DNN/ParallelNoQuantizationBufferedAsyncGradientAggregation/../../../DNN
MPI Rank 2: configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
MPI Rank 2: configparameters: cntk.cntk:deviceId=0
MPI Rank 2: configparameters: cntk.cntk:numCPUThreads=8
MPI Rank 2: configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: configparameters: cntk.cntk:parallelTrain=true
MPI Rank 2: configparameters: cntk.cntk:precision=double
MPI Rank 2: configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu
MPI Rank 2: configparameters: cntk.cntk:speechTrain=[
MPI Rank 2:     action = "train"
MPI Rank 2:     modelPath = "/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn"
MPI Rank 2:     deviceId = 0
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SimpleNetworkBuilder = [
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = "CrossEntropyWithSoftmax"
MPI Rank 2:         evalCriterion = "ErrorPrediction"
MPI Rank 2:         layerTypes = "Sigmoid"
MPI Rank 2:         initValueScale = 1.0
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         uniformInit = true
MPI Rank 2:         needPrior = true
MPI Rank 2:     ]
MPI Rank 2:     ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
MPI Rank 2:         layerSizes = 363:512:512:132
MPI Rank 2:         trainingCriterion = 'CE'
MPI Rank 2:         evalCriterion = 'Err'
MPI Rank 2:         applyMeanVarNorm = true
MPI Rank 2:         L = Length(layerSizes)-1    // number of model layers
MPI Rank 2:         features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
MPI Rank 2:         featNorm = if applyMeanVarNorm
MPI Rank 2:                    then MeanVarNorm(features)
MPI Rank 2:                    else features
MPI Rank 2:         layers[layer:1..L-1] = if layer > 1
MPI Rank 2:                                then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:                                else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
MPI Rank 2:         outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
MPI Rank 2:         outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
MPI Rank 2:         CE = if trainingCriterion == 'CE'
MPI Rank 2:              then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
MPI Rank 2:              else Fail('unknown trainingCriterion ' + trainingCriterion)
MPI Rank 2:         Err = if evalCriterion == 'Err' then
MPI Rank 2:               ErrorPrediction(labels, outZ, tag='evaluation')
MPI Rank 2:               else Fail('unknown evalCriterion ' + evalCriterion)
MPI Rank 2:         logPrior = LogPrior(labels)
MPI Rank 2:         // TODO: how to add a tag to an infix operation?
MPI Rank 2:         ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
MPI Rank 2:     ]
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize = 20480
MPI Rank 2:         minibatchSize = 64:256:1024
MPI Rank 2:         learningRatesPerMB = 1.0:0.5:0.1
MPI Rank 2:         numMBsToShowResult = 10
MPI Rank 2:         momentumPerMB = 0.9:0.656119
MPI Rank 2:         dropoutRate = 0.0
MPI Rank 2:         maxEpochs = 3
MPI Rank 2:         keepCheckPointFiles = true
MPI Rank 2:         clippingThresholdPerSample = 1#INF
MPI Rank 2:         ParallelTrain = [
MPI Rank 2:             parallelizationMethod = "DataParallelSGD"
MPI Rank 2:             distributedMBReading = true
MPI Rank 2:             DataParallelSGD = [
MPI Rank 2:                 gradientBits = 32
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2:         AutoAdjust = [
MPI Rank 2:             reduceLearnRateIfImproveLessThan = 0
MPI Rank 2:             loadBestModel = true
MPI Rank 2:             increaseLearnRateIfImproveMoreThan = 1000000000
MPI Rank 2:             learnRateDecreaseFactor = 0.5
MPI Rank 2:             learnRateIncreaseFactor = 1.382
MPI Rank 2:             autoAdjustLR = "adjustAfterEpoch"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2:     reader = [
MPI Rank 2:         readerType = "HTKMLFReader"
MPI Rank 2:         readMethod = "blockRandomize"
MPI Rank 2:         miniBatchMode = "partial"
MPI Rank 2:         randomize = "auto"
MPI Rank 2:         verbosity = 0
MPI Rank 2:         features = [
MPI Rank 2:             dim = 363
MPI Rank 2:             type = "real"
MPI Rank 2:             scpFile = "glob_0000.scp"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
MPI Rank 2:             labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
MPI Rank 2:             labelDim = 132
MPI Rank 2:             labelType = "category"
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2: ] [reader=[readerType=ExperimentalHTKMLFReader]] [reader=[prefetch=true]] [SGD=[ParallelTrain=[DataParallelSGD=[gradientBits=64]]]] [SGD=[ParallelTrain=[DataParallelSGD=[useBufferedAsyncGradientAggregation=true]]]] [SGD=[ParallelTrain=[parallelizationStartEpoch=2]]] [SGD=[maxEpochs=4]] [SGD=[ParallelTrain=[syncPerfStats=5]]]
MPI Rank 2: 
MPI Rank 2: configparameters: cntk.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/stderr
MPI Rank 2: configparameters: cntk.cntk:timestamping=true
MPI Rank 2: 05/03/2016 18:17:51: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 18:17:51: Commands: speechTrain
MPI Rank 2: 05/03/2016 18:17:51: Precision = "double"
MPI Rank 2: 05/03/2016 18:17:51: Using 8 CPU threads.
MPI Rank 2: 05/03/2016 18:17:51: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Speech/ExperimentalHtkmlfReader/DNN_ParallelNoQuantizationBufferedAsyncGradientAggregation@release_gpu/models/cntkSpeech.dnn
MPI Rank 2: 05/03/2016 18:17:51: CNTKCommandTrainInfo: speechTrain : 4
MPI Rank 2: 05/03/2016 18:17:51: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 4
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: ##############################################################################
MPI Rank 2: 05/03/2016 18:17:51: #                                                                            #
MPI Rank 2: 05/03/2016 18:17:51: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 18:17:51: #                                                                            #
MPI Rank 2: 05/03/2016 18:17:51: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: CNTKCommandTrainBegin: speechTrain
MPI Rank 2: SimpleNetworkBuilder Using GPU 0
MPI Rank 2: Reading script file glob_0000.scp ... 948 entries
MPI Rank 2: HTKDataDeserializer::HTKDataDeserializer: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 2: HTKDataDeserializer::HTKDataDeserializer: determined feature kind as 363-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 2: total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
MPI Rank 2: htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
MPI Rank 2: MLFDataDeserializer::MLFDataDeserializer: read 252734 sequences
MPI Rank 2: MLFDataDeserializer::MLFDataDeserializer: read 948 utterances
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Creating virgin network.
MPI Rank 2: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==8
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 7 roots:
MPI Rank 2: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
MPI Rank 2: 	EvalErrorPrediction = ErrorPrediction()
MPI Rank 2: 	InvStdOfFeatures = InvStdDev()
MPI Rank 2: 	MeanOfFeatures = Mean()
MPI Rank 2: 	PosteriorProb = Softmax()
MPI Rank 2: 	Prior = Mean()
MPI Rank 2: 	ScaledLogLikelihood = Minus()
MPI Rank 2: 
MPI Rank 2: Validating network. 25 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> labels = InputValue() :  -> [132 x *]
MPI Rank 2: Validating --> W2 = LearnableParameter() :  -> [132 x 512]
MPI Rank 2: Validating --> W1 = LearnableParameter() :  -> [512 x 512]
MPI Rank 2: Validating --> W0 = LearnableParameter() :  -> [512 x 363]
MPI Rank 2: Validating --> features = InputValue() :  -> [363 x *]
MPI Rank 2: Validating --> MeanOfFeatures = Mean (features) : [363 x *] -> [363]
MPI Rank 2: Validating --> InvStdOfFeatures = InvStdDev (features) : [363 x *] -> [363]
MPI Rank 2: Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [363 x *], [363], [363] -> [363 x *]
MPI Rank 2: Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [512 x 363], [363 x *] -> [512 x *]
MPI Rank 2: Validating --> B0 = LearnableParameter() :  -> [512 x 1]
MPI Rank 2: Validating --> W0*features+B0 = Plus (W0*features, B0) : [512 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 2: Validating --> H1 = Sigmoid (W0*features+B0) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 2: Validating --> W1*H1 = Times (W1, H1) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 2: Validating --> B1 = LearnableParameter() :  -> [512 x 1]
MPI Rank 2: Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 2: Validating --> H2 = Sigmoid (W1*H1+B1) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 2: Validating --> W2*H1 = Times (W2, H2) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
MPI Rank 2: Validating --> B2 = LearnableParameter() :  -> [132 x 1]
MPI Rank 2: Validating --> HLast = Plus (W2*H1, B2) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
MPI Rank 2: Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 2: Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 2: Validating --> PosteriorProb = Softmax (HLast) : [132 x 1 x *] -> [132 x 1 x *]
MPI Rank 2: Validating --> Prior = Mean (labels) : [132 x *] -> [132]
MPI Rank 2: Validating --> LogOfPrior = Log (Prior) : [132] -> [132]
MPI Rank 2: Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [132 x 1 x *], [132] -> [132 x 1 x *]
MPI Rank 2: 
MPI Rank 2: Validating network. 17 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 12 out of 25 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Created model with 25 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Training criterion node(s):
MPI Rank 2: 05/03/2016 18:17:51: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Evaluation criterion node(s):
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: 	EvalErrorPrediction = ErrorPrediction
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[EvalErrorPrediction Gradient[1]] [InvStdOfFeatures Gradient[363]] [LogOfPrior Gradient[132]] [MVNormalizedFeatures Gradient[363 x *]] [MeanOfFeatures Gradient[363]] [PosteriorProb Gradient[132 x 1 x *]] [PosteriorProb Value[132 x 1 x *]] [Prior Gradient[132]] [ScaledLogLikelihood Gradient[132 x 1 x *]] [features Gradient[363 x *]] [labels Gradient[132 x *]] }
MPI Rank 2: 0x2ca7648: {[features Value[363 x *]] }
MPI Rank 2: 0x395f668: {[MeanOfFeatures Value[363]] }
MPI Rank 2: 0x395fac8: {[InvStdOfFeatures Value[363]] }
MPI Rank 2: 0x3960818: {[W0 Value[512 x 363]] }
MPI Rank 2: 0x3a88f68: {[W1 Value[512 x 512]] }
MPI Rank 2: 0x3a89d38: {[B1 Value[512 x 1]] }
MPI Rank 2: 0x3a8aed8: {[W2 Value[132 x 512]] }
MPI Rank 2: 0x3a8bb88: {[B2 Value[132 x 1]] }
MPI Rank 2: 0x3a8c9b8: {[labels Value[132 x *]] }
MPI Rank 2: 0x3a8dc18: {[Prior Value[132]] }
MPI Rank 2: 0x3a934b8: {[EvalErrorPrediction Value[1]] }
MPI Rank 2: 0x3a937b8: {[ScaledLogLikelihood Value[132 x 1 x *]] }
MPI Rank 2: 0x3a93978: {[CrossEntropyWithSoftmax Value[1]] }
MPI Rank 2: 0x3a93e08: {[W0 Gradient[512 x 363]] [W0*features+B0 Value[512 x 1 x *]] }
MPI Rank 2: 0x3a93f78: {[LogOfPrior Value[132]] }
MPI Rank 2: 0x3a99578: {[B0 Value[512 x 1]] }
MPI Rank 2: 0x3d74c58: {[MVNormalizedFeatures Value[363 x *]] }
MPI Rank 2: 0x3d75418: {[W0*features Value[512 x *]] }
MPI Rank 2: 0x3d75628: {[H1 Value[512 x 1 x *]] [W0*features Gradient[512 x *]] }
MPI Rank 2: 0x3d75788: {[W0*features+B0 Gradient[512 x 1 x *]] [W1*H1 Value[512 x 1 x *]] }
MPI Rank 2: 0x3d758e8: {[W1 Gradient[512 x 512]] [W1*H1+B1 Value[512 x 1 x *]] }
MPI Rank 2: 0x3d75aa8: {[H2 Value[512 x 1 x *]] [W1*H1 Gradient[512 x 1 x *]] }
MPI Rank 2: 0x3d75c68: {[B0 Gradient[512 x 1]] [H1 Gradient[512 x 1 x *]] [W1*H1+B1 Gradient[512 x 1 x *]] [W2*H1 Value[132 x 1 x *]] }
MPI Rank 2: 0x3d75e28: {[HLast Value[132 x 1 x *]] [W2 Gradient[132 x 512]] }
MPI Rank 2: 0x3d76988: {[CrossEntropyWithSoftmax Gradient[1]] }
MPI Rank 2: 0x3d76b48: {[B1 Gradient[512 x 1]] [H2 Gradient[512 x 1 x *]] [HLast Gradient[132 x 1 x *]] }
MPI Rank 2: 0x3d76d08: {[W2*H1 Gradient[132 x 1 x *]] }
MPI Rank 2: 0x3d76ec8: {[B2 Gradient[132 x 1]] }
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: Precomputing --> 3 PreCompute nodes found.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:51: 	MeanOfFeatures = Mean()
MPI Rank 2: 05/03/2016 18:17:51: 	InvStdOfFeatures = InvStdDev()
MPI Rank 2: 05/03/2016 18:17:51: 	Prior = Mean()
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:54: Precomputing --> Completed.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:54: Starting Epoch 1: learning rate per sample = 0.015625  effective momentum = 0.900000  momentum as time constant = 607.4 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:54: Starting minibatch loop.
MPI Rank 2: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: CrossEntropyWithSoftmax = 4.40318406 * 640; EvalErrorPrediction = 0.90468750 * 640; time = 0.1911s; samplesPerSecond = 3348.3
MPI Rank 2: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: CrossEntropyWithSoftmax = 4.15980357 * 640; EvalErrorPrediction = 0.87187500 * 640; time = 0.0954s; samplesPerSecond = 6708.2
MPI Rank 2: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: CrossEntropyWithSoftmax = 3.98424210 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0957s; samplesPerSecond = 6689.6
MPI Rank 2: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: CrossEntropyWithSoftmax = 3.86209050 * 640; EvalErrorPrediction = 0.87656250 * 640; time = 0.0957s; samplesPerSecond = 6685.3
MPI Rank 2: 05/03/2016 18:17:54:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: CrossEntropyWithSoftmax = 3.80597620 * 640; EvalErrorPrediction = 0.88593750 * 640; time = 0.0958s; samplesPerSecond = 6683.2
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: CrossEntropyWithSoftmax = 3.73511552 * 640; EvalErrorPrediction = 0.87812500 * 640; time = 0.0957s; samplesPerSecond = 6686.0
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: CrossEntropyWithSoftmax = 3.57260725 * 640; EvalErrorPrediction = 0.81875000 * 640; time = 0.0957s; samplesPerSecond = 6688.1
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: CrossEntropyWithSoftmax = 3.42293687 * 640; EvalErrorPrediction = 0.80468750 * 640; time = 0.0957s; samplesPerSecond = 6685.1
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: CrossEntropyWithSoftmax = 3.34304309 * 640; EvalErrorPrediction = 0.76718750 * 640; time = 0.0958s; samplesPerSecond = 6679.5
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: CrossEntropyWithSoftmax = 3.37037793 * 640; EvalErrorPrediction = 0.84687500 * 640; time = 0.0946s; samplesPerSecond = 6762.0
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: CrossEntropyWithSoftmax = 3.21606065 * 640; EvalErrorPrediction = 0.76093750 * 640; time = 0.0954s; samplesPerSecond = 6709.6
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: CrossEntropyWithSoftmax = 3.31610118 * 640; EvalErrorPrediction = 0.78437500 * 640; time = 0.0956s; samplesPerSecond = 6694.8
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: CrossEntropyWithSoftmax = 3.14285888 * 640; EvalErrorPrediction = 0.75000000 * 640; time = 0.0955s; samplesPerSecond = 6703.1
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: CrossEntropyWithSoftmax = 3.01821991 * 640; EvalErrorPrediction = 0.70937500 * 640; time = 0.0964s; samplesPerSecond = 6640.1
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: CrossEntropyWithSoftmax = 3.01218944 * 640; EvalErrorPrediction = 0.73906250 * 640; time = 0.0957s; samplesPerSecond = 6688.1
MPI Rank 2: 05/03/2016 18:17:55:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: CrossEntropyWithSoftmax = 2.98947652 * 640; EvalErrorPrediction = 0.73593750 * 640; time = 0.0957s; samplesPerSecond = 6690.9
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: CrossEntropyWithSoftmax = 2.86297716 * 640; EvalErrorPrediction = 0.70000000 * 640; time = 0.0957s; samplesPerSecond = 6690.0
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: CrossEntropyWithSoftmax = 2.71901077 * 640; EvalErrorPrediction = 0.68593750 * 640; time = 0.0947s; samplesPerSecond = 6758.0
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: CrossEntropyWithSoftmax = 2.80860596 * 640; EvalErrorPrediction = 0.71250000 * 640; time = 0.0954s; samplesPerSecond = 6705.6
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: CrossEntropyWithSoftmax = 2.60590434 * 640; EvalErrorPrediction = 0.64687500 * 640; time = 0.0954s; samplesPerSecond = 6710.5
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: CrossEntropyWithSoftmax = 2.63920069 * 640; EvalErrorPrediction = 0.66875000 * 640; time = 0.0954s; samplesPerSecond = 6710.5
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: CrossEntropyWithSoftmax = 2.58372597 * 640; EvalErrorPrediction = 0.65781250 * 640; time = 0.0964s; samplesPerSecond = 6636.5
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: CrossEntropyWithSoftmax = 2.50997096 * 640; EvalErrorPrediction = 0.62031250 * 640; time = 0.0957s; samplesPerSecond = 6689.2
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: CrossEntropyWithSoftmax = 2.42126950 * 640; EvalErrorPrediction = 0.62968750 * 640; time = 0.0947s; samplesPerSecond = 6754.7
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: CrossEntropyWithSoftmax = 2.40125789 * 640; EvalErrorPrediction = 0.65156250 * 640; time = 0.0955s; samplesPerSecond = 6704.1
MPI Rank 2: 05/03/2016 18:17:56:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: CrossEntropyWithSoftmax = 2.47110816 * 640; EvalErrorPrediction = 0.63281250 * 640; time = 0.0954s; samplesPerSecond = 6708.2
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: CrossEntropyWithSoftmax = 2.33215267 * 640; EvalErrorPrediction = 0.60312500 * 640; time = 0.0939s; samplesPerSecond = 6815.1
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: CrossEntropyWithSoftmax = 2.21936103 * 640; EvalErrorPrediction = 0.56875000 * 640; time = 0.0950s; samplesPerSecond = 6739.1
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: CrossEntropyWithSoftmax = 2.31959580 * 640; EvalErrorPrediction = 0.61093750 * 640; time = 0.0950s; samplesPerSecond = 6739.2
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: CrossEntropyWithSoftmax = 2.19592881 * 640; EvalErrorPrediction = 0.61718750 * 640; time = 0.0950s; samplesPerSecond = 6740.0
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: CrossEntropyWithSoftmax = 2.28411654 * 640; EvalErrorPrediction = 0.60000000 * 640; time = 0.0949s; samplesPerSecond = 6743.9
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: CrossEntropyWithSoftmax = 2.18307184 * 640; EvalErrorPrediction = 0.55781250 * 640; time = 0.0786s; samplesPerSecond = 8140.7
MPI Rank 2: 05/03/2016 18:17:57: Finished Epoch[ 1 of 4]: [Training] CrossEntropyWithSoftmax = 2.99723568 * 20480; EvalErrorPrediction = 0.72426758 * 20480; totalSamplesSeen = 20480; learningRatePerSample = 0.015625; epochTime=3.17959s
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:57: Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.656119  momentum as time constant = 607.5 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:57: Starting minibatch loop, DataParallelSGD training (MyRank = 2, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 2: Actual gradient aggregation time: 0.017695
MPI Rank 2: Async gradient aggregation wait time: 0.000815
MPI Rank 2: Actual gradient aggregation time: 0.008493
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[   1-  10, 12.50%]: CrossEntropyWithSoftmax = 2.08990053 * 2304; EvalErrorPrediction = 0.56640625 * 2304; time = 0.1337s; samplesPerSecond = 17237.5
MPI Rank 2: Async gradient aggregation wait time: 0.010198
MPI Rank 2: Actual gradient aggregation time: 0.015352
MPI Rank 2: Async gradient aggregation wait time: 0.004744
MPI Rank 2: Actual gradient aggregation time: 0.01425
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  11-  20, 25.00%]: CrossEntropyWithSoftmax = 2.16003887 * 2560; EvalErrorPrediction = 0.58476562 * 2560; time = 0.1483s; samplesPerSecond = 17262.9
MPI Rank 2: Async gradient aggregation wait time: 0.005291
MPI Rank 2: Actual gradient aggregation time: 0.011922
MPI Rank 2: Async gradient aggregation wait time: 0.005256
MPI Rank 2: Actual gradient aggregation time: 0.015381
MPI Rank 2: 05/03/2016 18:17:57:  Epoch[ 2 of 4]-Minibatch[  21-  30, 37.50%]: CrossEntropyWithSoftmax = 2.19985756 * 2560; EvalErrorPrediction = 0.59492188 * 2560; time = 0.1433s; samplesPerSecond = 17866.0
MPI Rank 2: Async gradient aggregation wait time: 0.005076
MPI Rank 2: Actual gradient aggregation time: 0.014955
MPI Rank 2: Async gradient aggregation wait time: 0.005206
MPI Rank 2: Actual gradient aggregation time: 0.015138
MPI Rank 2: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  31-  40, 50.00%]: CrossEntropyWithSoftmax = 2.12388714 * 2560; EvalErrorPrediction = 0.57968750 * 2560; time = 0.1490s; samplesPerSecond = 17186.4
MPI Rank 2: Async gradient aggregation wait time: 0.00572
MPI Rank 2: Actual gradient aggregation time: 0.015121
MPI Rank 2: Async gradient aggregation wait time: 0.005176
MPI Rank 2: Actual gradient aggregation time: 0.015058
MPI Rank 2: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  41-  50, 62.50%]: CrossEntropyWithSoftmax = 2.05908444 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1525s; samplesPerSecond = 16783.3
MPI Rank 2: Async gradient aggregation wait time: 0.004624
MPI Rank 2: Actual gradient aggregation time: 0.014696
MPI Rank 2: Async gradient aggregation wait time: 0.00439
MPI Rank 2: Actual gradient aggregation time: 0.01521
MPI Rank 2: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  51-  60, 75.00%]: CrossEntropyWithSoftmax = 2.13603725 * 2560; EvalErrorPrediction = 0.57070312 * 2560; time = 0.1515s; samplesPerSecond = 16896.0
MPI Rank 2: Async gradient aggregation wait time: 0.004575
MPI Rank 2: Actual gradient aggregation time: 0.015683
MPI Rank 2: Async gradient aggregation wait time: 0.00521
MPI Rank 2: Actual gradient aggregation time: 0.015371
MPI Rank 2: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  61-  70, 87.50%]: CrossEntropyWithSoftmax = 2.09094421 * 2560; EvalErrorPrediction = 0.56406250 * 2560; time = 0.1505s; samplesPerSecond = 17013.9
MPI Rank 2: Async gradient aggregation wait time: 0.005028
MPI Rank 2: Actual gradient aggregation time: 0.015318
MPI Rank 2: Async gradient aggregation wait time: 0.005205
MPI Rank 2: Actual gradient aggregation time: 0.015055
MPI Rank 2: 05/03/2016 18:17:58:  Epoch[ 2 of 4]-Minibatch[  71-  80, 100.00%]: CrossEntropyWithSoftmax = 2.02829896 * 2560; EvalErrorPrediction = 0.56210938 * 2560; time = 0.1495s; samplesPerSecond = 17128.3
MPI Rank 2: Async gradient aggregation wait time: 0.006398
MPI Rank 2: Actual gradient aggregation time: 0.006532
MPI Rank 2: 05/03/2016 18:17:58: Finished Epoch[ 2 of 4]: [Training] CrossEntropyWithSoftmax = 2.10928672 * 20480; EvalErrorPrediction = 0.57392578 * 20480; totalSamplesSeen = 40960; learningRatePerSample = 0.001953125; epochTime=1.19376s
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:58: Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:58: Starting minibatch loop, DataParallelSGD training (MyRank = 2, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 2: Async gradient aggregation wait time: 0.024235
MPI Rank 2: Actual gradient aggregation time: 0.028554
MPI Rank 2: Async gradient aggregation wait time: 0.007245
MPI Rank 2: Actual gradient aggregation time: 0.029245
MPI Rank 2: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.99429576 * 9216; EvalErrorPrediction = 0.54709201 * 9216; time = 0.2692s; samplesPerSecond = 34239.7
MPI Rank 2: Async gradient aggregation wait time: 0.002789
MPI Rank 2: Actual gradient aggregation time: 0.028375
MPI Rank 2: Async gradient aggregation wait time: 0.005983
MPI Rank 2: Actual gradient aggregation time: 0.024917
MPI Rank 2: 05/03/2016 18:17:59:  Epoch[ 3 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.92530640 * 10240; EvalErrorPrediction = 0.52812500 * 10240; time = 0.2555s; samplesPerSecond = 40073.7
MPI Rank 2: 05/03/2016 18:17:59: Finished Epoch[ 3 of 4]: [Training] CrossEntropyWithSoftmax = 1.95886100 * 20480; EvalErrorPrediction = 0.53725586 * 20480; totalSamplesSeen = 61440; learningRatePerSample = 9.7656251e-05; epochTime=0.543637s
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:59: Starting Epoch 4: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:59: Starting minibatch loop, DataParallelSGD training (MyRank = 2, NumNodes = 3, NumGradientBits = 64), BufferedAsyncGradientAggregation is ENABLED, distributed reading is ENABLED.
MPI Rank 2: Async gradient aggregation wait time: 0.013656
MPI Rank 2: Actual gradient aggregation time: 0.030699
MPI Rank 2: Async gradient aggregation wait time: 0.000605
MPI Rank 2: Actual gradient aggregation time: 0.024596
MPI Rank 2: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[   1-  10, 50.00%]: CrossEntropyWithSoftmax = 1.90011431 * 9216; EvalErrorPrediction = 0.51725260 * 9216; time = 0.2712s; samplesPerSecond = 33987.9
MPI Rank 2: Async gradient aggregation wait time: 0.009065
MPI Rank 2: Actual gradient aggregation time: 0.029622
MPI Rank 2: Async gradient aggregation wait time: 6e-06
MPI Rank 2: Actual gradient aggregation time: 0.012457
MPI Rank 2: 05/03/2016 18:17:59:  Epoch[ 4 of 4]-Minibatch[  11-  20, 100.00%]: CrossEntropyWithSoftmax = 1.88429973 * 10240; EvalErrorPrediction = 0.52099609 * 10240; time = 0.2740s; samplesPerSecond = 37366.5
MPI Rank 2: Async gradient aggregation wait time: 0.006131
MPI Rank 2: 05/03/2016 18:17:59: Finished Epoch[ 4 of 4]: [Training] CrossEntropyWithSoftmax = 1.89248911 * 20480; EvalErrorPrediction = 0.51933594 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 9.7656251e-05; epochTime=0.56187s
MPI Rank 2: 05/03/2016 18:17:59: CNTKCommandTrainEnd: speechTrain
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:17:59: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:18:00: __COMPLETED__